{
  
    
        "post0": {
            "title": "The Machine Learning Toolbox",
            "content": "Communities . ML Collective . The ML Collective is a nonprofit organization that connects you with other machine learning practicioners. . Link to ML Collective . Kaggle . The primary supervised learning machine learning competition platform . Link to Kaggle . Concepts . Degenerate feedback loops . Predictions influence feedback, where the feedback is used to extract labels (e.g. recommender systems that propose popular items based on how often they’re clicked), | Detect degenerate feedback loops using aggregate diversity or average coverage of long tail items, | Introduce randomization into recommendations / predictions to gather more realistic feedback (downside user experience), | Capture features of popularity (e.g. position in recommendation list) for prediction model, | . Further reading . Write-up by Chip Huyen | . Data distribution shift . For our machine learning model we call the inputs X and the outouts Y. The training data in supervised learning is a sample of the (unknown) joint distribution P(X, Y). In machine learning we usually model P(Y|X) - i.e. the conditional probability of the output given some observed input. . P(X, Y) = P(X | Y) P(Y) = P(Y | X) P(X) | . Covariate shift: P(X) changes while P(Y | X) is unchanged (distribution of the input changes but the distribution of the output given the input is unchanged) | . | Label shift: P(Y) changes while P(X | Y) is unchanged | . | Concept drift: P(Y | X) changes while P(X) is unchanged | . | . Further reading . Write-up by Chip Huyen | . Frameworks . Pandas . Pandas is the primary data manipulation framework for data scientists in Python. It entails and operates on two primary data models: Series, one-dimensional data / table columns, and dataframes, two-dimensional data akin to tables. . When (not) to use it . Use when the data you’re manipulating fit in memory | . Further reading . Link to pandas documentation | Python for Data Analysis, 3rd edition | .",
            "url": "https://georg.io/mltoolbox",
            "relUrl": "/mltoolbox",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Causal inference for decision-making in growth hacking and upselling in Python",
            "content": "Introduction . Wow, growth hacking and upselling all in the same article? Also Python. . Okay, let&#39;s start at the beginning. Imagine the following scenario: You&#39;re responsible for increasing the amount of money users spend on your e-commerce platform. . You and your team come up with different measures you could implement to achieve your goal. Two of these measures could be: . Provide a discount on your best-selling items, | Implement a rewards program that incentivices repeat purchases. | . Both of these measures are fairly complex with each incurring a certain, probably known, amount of cost and an unknown effect on your customers&#39; spending behaviour. . To decide which of these two possible measures is worth both the effort and incurred cost you need to estimate their effect on customer spend. . A natural way of estimating this effect is computing the following: . $ textrm{avg}( textrm{spend} | textrm{treatment} = 1) - textrm{avg}( textrm{spend} | textrm{treatment} = 0) = textrm{ATE}$. . Essentially you would compute the average spend of users who received the treatment (received a discount or signed up for rewards) and subtract from that the average spend of users who didn&#39;t receive the treatment. . Without discussing the details of the underlying potential outcomes framework, the above expression is called the average treatment effect (ATE). . Let&#39;s estimate the average treatment effect and make a decision! . So now we&#39;ll just analyze our e-commerce data of treated and untreated customers and compute the average treatment effect (ATE) for each proposed measure, right? Right? . Before you rush ahead with your ATE computations - now is a good time to take a step back and contemplate how your data was generated in the first place(data-generating process). . References and further material ... . Before we continue: My example here is based on a tutorial by the authors of the excellent DoWhy library. You can find the original tutorial here: . https://github.com/microsoft/dowhy/blob/master/docs/source/example_notebooks/dowhy_example_effect_of_memberrewards_program.ipynb . And more on DoWhy here: https://microsoft.github.io/dowhy/ . Install and load libraries . !pip install dowhy --quiet . import random import pandas as pd import numpy as np np.random.seed(42) random.seed(42) . Randomized controlled trial / experimental data . So where were we ... ah right! Where does our e-commerece data come from? . Since we don&#39;t actually run an e-commerce operation here we will have to simulate our data (remember: these ideas are based on the above DoWhy tutorial). . Imagine we observe the monthly spend of each of our 10,000 users over the course of a year. Each user will spend with a certain distribution (here, a Poisson distribution) and there are both high and low spenders with different mean spends. . Over the course of the year, each user can sign up to our rewards program in any month and once they have signed up their spend goes up by 50% relative to what they would&#39;ve spent without signing up. . So far so mundane: Different customers show different spending behaviour and signing up to our rewards program increases their spend. . Now the big question is: How are treatment assignment (rewards program signup) and outcome (spending behaviour) related? . If treatment and outcome, interpreted as random variables, are independent of one another then according to the potential outcome framework we can compute the ATE as easily as shown above: . $ textrm{ATE} = textrm{avg}( textrm{spend} | textrm{treatment} = 1) - textrm{avg}( textrm{spend} | textrm{treatment} = 0)$ . When are treatment and outcome independent? The gold standard for achieving their independence in a data set is the randomized controlled trial (RCT). . In our scenario what an RCT would look like is randomly signing up our users to our rewards program - indepndent of their spending behaviour or any other characteristic. . So we would go through our list of 10,000 users and flip a coin for each of them, sign them up to our program in a random month of the year based on our coin, and send them on their merry way to continue buying stuff in our online shop. . Let&#39;s put all of this into a bit of code that simulates the spending behaviour of our users according to our thought experiment: . num_users = 10000 num_months = 12 df = pd.DataFrame({ &#39;user_id&#39;: np.repeat(np.arange(num_users), num_months), &#39;month&#39;: np.tile(np.arange(1, num_months+1), num_users), # months are from 1 to 12 &#39;high_spender&#39;: np.repeat(np.random.randint(0, 2, size=num_users), num_months), }) df[&#39;spend&#39;] = None df.loc[df[&#39;high_spender&#39;] == 0, &#39;spend&#39;] = np.random.poisson(250, df.loc[df[&#39;high_spender&#39;] == 0].shape[0]) df.loc[df[&#39;high_spender&#39;] == 1, &#39;spend&#39;] = np.random.poisson(750, df.loc[df[&#39;high_spender&#39;] == 1].shape[0]) df[&quot;spend&quot;] = df[&quot;spend&quot;] - df[&quot;month&quot;] * 10 signup_months = np.random.choice( np.arange(1, num_months), num_users ) * np.random.randint(0, 2, size=num_users) # signup_months == 0 means customer did not sign up df[&#39;signup_month&#39;] = np.repeat(signup_months, num_months) # A customer is in the treatment group if and only if they signed up df[&quot;treatment&quot;] = df[&quot;signup_month&quot;] &gt; 0 # Simulating a simple treatment effect of 50% after_signup = (df[&quot;signup_month&quot;] &lt; df[&quot;month&quot;]) &amp; (df[&quot;treatment&quot;]) df.loc[after_signup, &quot;spend&quot;] = df[after_signup][&quot;spend&quot;] * 1.5 . Let&#39;s look at user 0 and their treatment assignment as well as spend (since we&#39;re sampling random variables here you&#39;ll see something different from me): . df.loc[df[&#39;user_id&#39;] == 0] . user_id month high_spender spend signup_month treatment . 0 0 | 1 | 0 | 235 | 0 | False | . 1 0 | 2 | 0 | 249 | 0 | False | . 2 0 | 3 | 0 | 240 | 0 | False | . 3 0 | 4 | 0 | 224 | 0 | False | . 4 0 | 5 | 0 | 184 | 0 | False | . 5 0 | 6 | 0 | 172 | 0 | False | . 6 0 | 7 | 0 | 182 | 0 | False | . 7 0 | 8 | 0 | 155 | 0 | False | . 8 0 | 9 | 0 | 120 | 0 | False | . 9 0 | 10 | 0 | 153 | 0 | False | . 10 0 | 11 | 0 | 148 | 0 | False | . 11 0 | 12 | 0 | 159 | 0 | False | . Average treatment effect on post-signup spend for experimental data . The effect we&#39;re interested in is the impact of rewards signup on spending behaviour - i.e. the effect on post-signup spend. . Since customers can sign up any month of the year, we&#39;ll choose one month at random and compute the effect with respect to that one month. . So let&#39;s create a new table from our time series where we collect post-signup spend for those customers that signed up in month = 6 alongside the spend of customers who never signed up. . month = 6 post_signup_spend = ( df[df.signup_month.isin([0, month])] .groupby([&quot;user_id&quot;, &quot;signup_month&quot;, &quot;treatment&quot;]) .apply( lambda x: pd.Series( { &quot;post_spend&quot;: x.loc[x.month &gt; month, &quot;spend&quot;].mean(), } ) ) .reset_index() ) print(post_signup_spend) . user_id signup_month treatment post_spend 0 0 0 False 152.833333 1 3 0 False 162.166667 2 4 0 False 146.333333 3 6 0 False 153.666667 4 7 6 True 240.750000 ... ... ... ... ... 5451 9990 0 False 629.833333 5452 9993 0 False 674.500000 5453 9994 0 False 681.000000 5454 9995 0 False 641.333333 5455 9998 0 False 658.833333 [5456 rows x 4 columns] . To get the average treatment effect (ATE) of our rewards signup treatment we now compute the average post-signup spend of the customers who signed up and subtract from that the average spend of users who didn&#39;t sign up: . post_spend = post_signup_spend .groupby(&#39;treatment&#39;) .agg({&#39;post_spend&#39;: &#39;mean&#39;}) post_spend . post_spend . treatment . False 403.512239 | . True 610.140371 | . So the ATE of rewards signup on post-signup spend is: . post_spend.loc[True, &#39;post_spend&#39;] - post_spend.loc[False, &#39;post_spend&#39;] . 206.62813242372852 . Since we simulated the treatment effect ourselves (50% post-signup spend increase) let&#39;s see if we can recover this effect from our data: . post_spend.loc[True, &#39;post_spend&#39;] / post_spend.loc[False, &#39;post_spend&#39;] . 1.5120740154875112 . The post-signup spend for treated customers is roughly 50% greater than the spend for untreated customers - exactly the treatment effect we simulated! . Remember, however, that we are dealing with clean experimental data from a randomized controlled trial (RCT) here! The potential outcome framework tells us that for data from an RCT the simple ATE formula we used here yields the correct treatment effect due to independence of treatment assignment and outcome. . So the fact that we recovered the actual (simulated) treatment effect is nice to see but not surprising. . The issue with randomized controlled trials and observational data . Our above thought experiment where we randomly assigned our customers to our rewards program isn&#39;t very realistic. . Randomly signing up paying customers to rewards programs without their consent may upset some and may not even be permissible. The same issue with randomized treatment assignment pops up everywhere - clean randomized controlled trials are oftentimes too expensive, infeasible to implement, unethical, or not permitted. . But since we still need to experiment with our shop to drive spending behaviour we&#39;ll still go ahead and implement our rewards program. Only that this time we&#39;ll place a regular signup page in our shop where our customers can decide for themselves if they want to sign up or not. . Activating our signup page and simply observing how users and their spend behaves gives us observational data. . We usually call &quot;observational data&quot; just &quot;data&quot; without giving much thought to where they came from. I mean we&#39;ve all dealt with lots of different kinds of data (marketing data, R&amp;D measurements, HR data, etc.) and all these data were simply &quot;observed&quot; and didn&#39;t come out of a carefully set up experiment. . Simulating our observational data we&#39;ve got the same 10,000 customers over a span of a year. We still have the same high and low spenders. . Only that now our high spenders are far more likely to sign up to our rewards program than our low spenders. My reasoning for this is that customers who spend more are also more likely to show greater brand loyalty towards us and our rewards program. Further, they visit our shop more frequently hence are more likely to notice our new rewards program and the signup page. We could also add this behaviour as random variables to our simulation below but just take a shortcut and give low spenders a 5% chance of signing up and high spenders a 95% chance. . num_users = 10000 num_months = 12 df = pd.DataFrame({ &#39;user_id&#39;: np.repeat(np.arange(num_users), num_months), &#39;month&#39;: np.tile(np.arange(1, num_months+1), num_users), # months are from 1 to 12 &#39;high_spender&#39;: np.repeat(np.random.randint(0, 2, size=num_users), num_months), }) df[&#39;spend&#39;] = None df.loc[df[&#39;high_spender&#39;] == 0, &#39;spend&#39;] = np.random.poisson(250, df.loc[df[&#39;high_spender&#39;] == 0].shape[0]) df.loc[df[&#39;high_spender&#39;] == 1, &#39;spend&#39;] = np.random.poisson(750, df.loc[df[&#39;high_spender&#39;] == 1].shape[0]) signup_months = df[[&#39;user_id&#39;, &#39;high_spender&#39;]].drop_duplicates().copy() signup_months[&#39;signup_month&#39;] = None signup_months.loc[signup_months[&#39;high_spender&#39;] == 0, &#39;signup_month&#39;] = np.random.choice( np.arange(1, num_months), (signup_months[&#39;high_spender&#39;] == 0).sum() ) * np.random.binomial(1, .05, size=(signup_months[&#39;high_spender&#39;] == 0).sum()) signup_months.loc[signup_months[&#39;high_spender&#39;] == 1, &#39;signup_month&#39;] = np.random.choice( np.arange(1, num_months), (signup_months[&#39;high_spender&#39;] == 1).sum() ) * np.random.binomial(1, .95, size=(signup_months[&#39;high_spender&#39;] == 1).sum()) df = df.merge(signup_months) df[&quot;treatment&quot;] = df[&quot;signup_month&quot;] &gt; 0 after_signup = (df[&quot;signup_month&quot;] &lt; df[&quot;month&quot;]) &amp; (df[&quot;treatment&quot;]) df.loc[after_signup, &quot;spend&quot;] = df[after_signup][&quot;spend&quot;] * 1.5 df . user_id month high_spender spend signup_month treatment . 0 0 | 1 | 1 | 778 | 11 | True | . 1 0 | 2 | 1 | 726 | 11 | True | . 2 0 | 3 | 1 | 704 | 11 | True | . 3 0 | 4 | 1 | 723 | 11 | True | . 4 0 | 5 | 1 | 718 | 11 | True | . ... ... | ... | ... | ... | ... | ... | . 119995 9999 | 8 | 1 | 745 | 0 | False | . 119996 9999 | 9 | 1 | 777 | 0 | False | . 119997 9999 | 10 | 1 | 776 | 0 | False | . 119998 9999 | 11 | 1 | 744 | 0 | False | . 119999 9999 | 12 | 1 | 768 | 0 | False | . 120000 rows × 6 columns . Now imagine you weren&#39;t aware of causality, confounders, high / low spenders, and all that. You simply published your rewards signup page and observed your customers&#39; spending behaviour over a span of a year. Chances are you&#39;ll compute the average treatment effect the exact same way we did above for our randomized controlled trial: . month = 6 post_signup_spend = ( df[df.signup_month.isin([0, month])] .groupby([&quot;user_id&quot;, &quot;signup_month&quot;, &quot;treatment&quot;]) .apply( lambda x: pd.Series( { &quot;post_spend&quot;: x.loc[x.month &gt; month, &quot;spend&quot;].mean(), &quot;pre_spend&quot;: x.loc[x.month &lt; month, &quot;spend&quot;].mean(), } ) ) .reset_index() ) print(post_signup_spend) . user_id signup_month treatment post_spend pre_spend 0 1 0 False 251.666667 239.0 1 3 0 False 246.166667 252.8 2 4 0 False 740.833333 737.2 3 6 0 False 254.333333 247.0 4 7 0 False 249.166667 253.2 ... ... ... ... ... ... 5499 9992 0 False 246.000000 240.6 5500 9995 0 False 254.833333 256.4 5501 9996 0 False 248.833333 239.8 5502 9997 0 False 249.500000 247.8 5503 9999 0 False 761.666667 752.6 [5504 rows x 5 columns] . post_spend = post_signup_spend .groupby(&#39;treatment&#39;) .agg({&#39;post_spend&#39;: &#39;mean&#39;}) post_spend . post_spend . treatment . False 275.891760 | . True 1075.543699 | . post_spend.loc[True, &#39;post_spend&#39;] - post_spend.loc[False, &#39;post_spend&#39;] . 799.6519394104552 . post_spend.loc[True, &#39;post_spend&#39;] / post_spend.loc[False, &#39;post_spend&#39;] . 3.8984263250854134 . Performing the exact same computation as above, now we&#39;re estimating an average treatment effect of almost 400% instead of the actual 50%! . So what went wrong here? . Observational data got us! . Realize that in our observational data the outcome (spend) is not indepndent of treatment assignmnet (rewards program signup): High spenders are far more likely to sign up hence are overrepresented in our treatment group while low spenders are overrepresented in our control group (users that didn&#39;t sign up). . So when we compute the above difference or ratio we don&#39;t just see the average treatment effect of rewards signup we also see the inherent difference in spending between high and low spenders. . So if we ignore how our observational data are generated we&#39;ll overestimate the effect our rewards program has and likely make decisions that seem to be supported by data but in reality aren&#39;t. . Also notice that we often make this same mistake when training machine learning algorithms on observational data. Chances are someone will ask you to train a regression model to predict the effectiveness of the rewards program and your model will end up with the same inflated estimate as above. . So how do we fix this? And how can we estimate the true treatment effect from our observational data? . Generally, we know from experience in e-commerece that people who tend to spend more are more likely to sign up to our rewards program. So we could segment our users into spend buckets and compute the treatment effect within each bucket to try and breeak this confounding link in our observational data. . Notice that in practice we won&#39;t have a high spender flag for our customers so we&#39;ll have to go by our customers&#39; observed spending behaviour. . The causal inference framework offers an established approach here: Relying on our domain knowledge, we define a causal model that describes how we believe our observational data were generated. . Let&#39;s draw this as a graph with nodes and edges: . import os, sys sys.path.append(os.path.abspath(&quot;../../../&quot;)) import dowhy causal_graph = &quot;&quot;&quot;digraph { treatment[label=&quot;Program Signup in month i&quot;]; pre_spend; post_spend; U[label=&quot;Unobserved Confounders&quot;]; pre_spend -&gt; treatment; pre_spend -&gt; post_spend; treatment-&gt;post_spend; U-&gt;treatment; U-&gt;pre_spend; U-&gt;post_spend; }&quot;&quot;&quot; model = dowhy.CausalModel( data=post_signup_spend, graph=causal_graph.replace(&quot; n&quot;, &quot; &quot;), treatment=&quot;treatment&quot;, outcome=&quot;post_spend&quot; ) model.view_model() . Our causal model states what we described above: Pre-signup spend influences both rewards signup (treatment assignment) and post-signup spend. This is the story about our high and low spenders. . Treatment (rewards signup) influences post-signup spending behaviour - this is the effect we&#39;re actually interested in. . We also added a node U to signify possible other confounding factors that may exist in reality but weren&#39;t observed as part of our data. . Identification / identifying the causal effect . We will now apply do-calculus to our causal model from above to figure out ways to cleanly estimate the treatment effect we&#39;re after: . identified_estimand = model.identify_effect(proceed_when_unidentifiable=True) print(identified_estimand) . Estimand type: nonparametric-ate ### Estimand : 1 Estimand name: backdoor Estimand expression: d ────────────(Expectation(post_spend|pre_spend)) d[treatment] Estimand assumption 1, Unconfoundedness: If U→{treatment} and U→post_spend then P(post_spend|treatment,pre_spend,U) = P(post_spend|treatment,pre_spend) ### Estimand : 2 Estimand name: iv No such variable found! ### Estimand : 3 Estimand name: frontdoor No such variable found! . Very broadly and sloppily stated there a three ways to segment (or slice and dice) our observational data to get to subsets of our data within which we can cleanly compute the average treatment effect: . Backdoor adjustment, | Frontdoor adjustment, and | Instrumental variables. | . The above printout tells us that based on both the causal model we constructed and our observational data there is a backdoor-adjusted estimator for our desired treatment effect. . This backdoor adjustment actually follows closely what we already said above: We&#39;ll compute the post-spend given pre-spend (segment our customers based on their spending behaviour). . Estimating the treatment effect . There are various ways to perform the backdoor adjustment that DoWhy identified for us above. One of them is called propensity score matching: . estimate = model.estimate_effect( identified_estimand, method_name=&quot;backdoor1.propensity_score_matching&quot;, target_units=&quot;ate&quot; ) print(estimate) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) . *** Causal Estimate *** ## Identified estimand Estimand type: nonparametric-ate ## Realized estimand b: post_spend~treatment+pre_spend Target units: ate ## Estimate Mean value: 159.15597747093042 . DoWhy provides us with an estimated ATE for our observational data that is pretty close to the ATE we computed for our experimental data from our randomized controlled trial. . Even if the ATE estimate DoWhy provides doesn&#39;t match exactly our experimental ATE we&#39;re now in a much better position to take a decision regarding our rewards program based on our observational data. . So next time we want to base decisions on observational data it&#39;ll be worthwhile defining a causal model of the underlying data-generating process and using a library such as DoWhy that helps us identify and apply adjustment strategies. .",
            "url": "https://georg.io/2021/08/18/Causal_inference_for_decision_making_in_growth_hacking_and_upselling_in_Python",
            "relUrl": "/2021/08/18/Causal_inference_for_decision_making_in_growth_hacking_and_upselling_in_Python",
            "date": " • Aug 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Causality modelling in Python for data scientists",
            "content": "import pandas as pd import numpy as np import random . np.random.seed(123) random.seed(123) . no_samples = 10000 seasons = np.random.choice([&#39;winter&#39;, &#39;spring&#39;, &#39;summer&#39;, &#39;fall&#39;], size=(no_samples,)) color = np.array( [ random.choice([&#39;yellow&#39;, &#39;pink&#39;]) if season in [&#39;spring&#39;, &#39;summer&#39;] else random.choice([&#39;navy&#39;, &#39;grey&#39;]) for season in seasons ] ) price = np.random.lognormal(size=(no_samples,)) rank = np.array( [ ] ) . seasons . array([&#39;fall&#39;, &#39;spring&#39;, &#39;spring&#39;, ..., &#39;spring&#39;, &#39;spring&#39;, &#39;spring&#39;], dtype=&#39;&lt;U6&#39;) .",
            "url": "https://georg.io/2020/07/20/Causality_modelling_for_beginners_in_Python",
            "relUrl": "/2020/07/20/Causality_modelling_for_beginners_in_Python",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Causality modelling in Python for data scientists",
            "content": "The need for causality modelling . Intelligent planning and decision-making lie at the heart of most business success. . The decisions that our business needs to evaluate can range from those that are relatively low effort and we take potentially thousands or millions of times a day to those that are high effort and are taken every couple of months: . What will happen if I show an advertising banner to a particular user? | What will happen if I change the retail prices for certain products in my shop? | What will happen if I alter my manufacturing process? | What will happen if I swap out a particular mechanical piece in a vehicle I develop? | What will happen if I invest in new property, machinery, or processes? | What will happen if I hire this applicant? | What if I increase remuneration of my workforce? | As industrial data scientists we are oftentimes called upon to evaluate these proposed business decisions using analytics, machine learning methodologies, and past data. . What we may end up doing for the above proposed business decisions is: . Compute and rank past click-through rates for given pairs of ad banner and user, | Correlate past demand with set retail prices for product groups of interest, | Correlate past manufacturing parameters with achieved output quality, | Correlate the mechanical behavior of my vehicles with the mechnical parts used in it, | Use past data to forecast the development of real estate prices, | Use past data to correlate and predict the productivity of my team given e.g. its size or makeup, and | Use past data to correlate productivity and remuneration levels. | The way I formulated these is already pretty suggestive - but essentially some of our common approaches to evaluating business decisions do not compare our business outcomes with and without said business decisions but they rather look at our data outside the context of decision-making. . Put another way, we oftentimes analyze past data without considering the state our business or customer is in when those data were generated. For illustration: . . So really when tasked with evaluating the above proposed business decisions we should instead think in terms of questions akin the following: . How would the user of interest behave differently if we didn&#39;t show them (and pay for) a banner now? | For each Euro we shave off a price tag how much higher will our revenue be since more customers are inclinced to place an order? | 4. 5. 6. 7. | How to do causality modelling . The authors Hünermund and Bareinboim (https://arxiv.org/abs/1912.09104) proposed a methodology they called data-fusion process. . The data-fusion process maps out the individual steps necessary for evaluating the impact of past and potential future decisions: . . Use case: The impact of direct marketing on customer behavior . We&#39;ll use a data set provided by UCI (https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) that demonstrates the potential impact of direct marketing on customer success. . Let&#39;s dive right in, download the data set and see what we are working with. . The direct marketing success data set . !wget --quiet https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip . !unzip -oqq bank.zip . import pandas as pd . df = pd.read_csv(&#39;bank.csv&#39;, delimiter=&#39;;&#39;) df[&#39;success&#39;] = df[&#39;y&#39;] del df[&#39;y&#39;] df[&#39;success&#39;] = df[&#39;success&#39;].replace(&#39;no&#39;, 0) df[&#39;success&#39;] = df[&#39;success&#39;].replace(&#39;yes&#39;, 1) del df[&#39;duration&#39;] df[&#39;no_contacts&#39;] = df[&#39;campaign&#39;] del df[&#39;campaign&#39;] . . df.head() . age job marital education default balance housing loan contact day month pdays previous poutcome success no_contacts . 0 30 | unemployed | married | primary | no | 1787 | no | no | cellular | 19 | oct | -1 | 0 | unknown | 0 | 1 | . 1 33 | services | married | secondary | no | 4789 | yes | yes | cellular | 11 | may | 339 | 4 | failure | 0 | 1 | . 2 35 | management | single | tertiary | no | 1350 | yes | no | cellular | 16 | apr | 330 | 1 | failure | 0 | 1 | . 3 30 | management | married | tertiary | no | 1476 | yes | yes | unknown | 3 | jun | -1 | 0 | unknown | 0 | 4 | . 4 59 | blue-collar | married | secondary | no | 0 | yes | no | unknown | 5 | may | -1 | 0 | unknown | 0 | 1 | . Our tabular marketing and sales data shows a number of features we observe about a given customer and our interaction with them: . The customer&#39;s age, job, marital status, education, current account balance, and whether or not they already took out a loan are recorded, | Our direct marketing interaction with a given customer is also recorded, for instance, how often we already contacted them. | . A more detailed description of the features in our data can be found here: . https://archive.ics.uci.edu/ml/datasets/Bank+Marketing . Trying to help our business with machine learning only . target = &#39;success&#39; features = [column for column in df.columns if column != target] . import lightgbm as lgb from sklearn.preprocessing import OrdinalEncoder . model = lgb.LGBMClassifier() . X, y = df[features], df[target] . numerical_features = [&#39;age&#39;, &#39;balance&#39;, &#39;no_contacts&#39;, &#39;previous&#39;, &#39;pdays&#39;] categorical_features = [feature for feature in features if feature not in numerical_features] . encoder = OrdinalEncoder(dtype=int) . X_numeric = pd.concat( [ X[numerical_features], pd.DataFrame( data=encoder.fit_transform(X[categorical_features]), columns=categorical_features ) ], axis=1 ) . X_numeric.head() . age balance no_contacts previous pdays job marital education default housing loan contact day month poutcome . 0 30 | 1787 | 1 | 0 | -1 | 10 | 1 | 0 | 0 | 0 | 0 | 0 | 18 | 10 | 3 | . 1 33 | 4789 | 1 | 4 | 339 | 7 | 1 | 1 | 0 | 1 | 1 | 0 | 10 | 8 | 0 | . 2 35 | 1350 | 1 | 1 | 330 | 4 | 2 | 2 | 0 | 1 | 0 | 0 | 15 | 0 | 0 | . 3 30 | 1476 | 4 | 0 | -1 | 4 | 1 | 2 | 0 | 1 | 1 | 2 | 2 | 6 | 3 | . 4 59 | 0 | 1 | 0 | -1 | 1 | 1 | 1 | 0 | 1 | 0 | 2 | 4 | 8 | 3 | . model.fit(X_numeric, y) . LGBMClassifier() . %matplotlib inline . lgb.plot_importance(model); . There are numerous ways to compute feature importance and this one implemented in the LightGBM library measures the number of times a given feature is used in the constructed trees: . https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_importance.html . In general, feature importance gives us a measure of how well a given measured variable correlates with the target (marketing success in our case). . The question here is: How can we use our trained success predictor and our feature importances to aid intelligent plannning and decision-making in our business? . Uses for .",
            "url": "https://georg.io/2020/06/20/Causality_modelling_python",
            "relUrl": "/2020/06/20/Causality_modelling_python",
            "date": " • Jun 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "The Causality-Driven Company",
            "content": "The data-driven company . Much has been written on data being the new oil and enterprises needing to embrace big data and machine learning to benefit in three important ways: . Utilizing existing resources more efficiently thus reducing cost, | Increasing success rates thus increasing revenue, and | Developing entirely new revenue streams thus diversifying their business. | Enterprises are said to be data-driven once their entire organization is data-first, meaning: . Both their core processes (e.g. production lines) and supporting processes (e.g. recruiting) are quantifiable through transparent metrics, and | All business units have a data mindset, i.e. are driven by testable hypotheses, and have the technological means for it. | Machine learning applications for the data-driven company . There a numerous applications of big data and machine learning in industry and the enterprise. Common applications include: . Predictive maintenance, | Demand forecasting, | Optimal pricing, | Intelligent routing, | Data-driven customer support (e.g. chatbots), amd | Churn prediction. | . Another key application in industry is smart manufacturing which promises to decrease cost and increase both production line output and quality. . Smart manufacturing . Product quality prediction . . Consider our production line, pictured above in Figure 1, which consists of a number of workstations that the different parts of our product need to pass through before being assembled into our final product. . The various workstations can be described by parameters, e.g. how much pressure or force is applied, how fast a tool is spinning, or categories such as the type of drill used. . Our fictional production line above has four production parameters spread across its workstations: AAA, BBB, CCC, and DDD. . At the far end of our production line the quality of our assembled product is quantified in the quality assurance gateway (QA verb!QA!QA). Here, either an employee or a machine visually inspects or physically tests manufactured items to verify their quality before they are shipped to customers. . Since our company is data-driven we collect data on every item coming off our production line. Such a data set may look as follows: . Product ID Timestamp A B C D QA . … | … | … | … | … | … | … | . 123 | 2020-05-10 14:35:00 | 1.2 | 102.1 | ZZ | YY | 0.98 | . 456 | 2020-05-10 16:20:00 | 1.1 | 104.3 | ZZ | XX | 0.96 | . 789 | 2020-05-11 06:04:00 | 1.3 | 100.6 | ZX | YY | 0.83 | . 012 | 2020-05-11 10:45:00 | 1.1 | 101.3 | ZX | YZ | 0.92 | . … | … | … | … | … | … | … | . Parameters AAA and BBB of our production line are numerical features while parameters CCC and DDD are categorical features. Our quality measurement QA verb!QA!QA is a numerical target variable which takes values between 0 and 1 with the latter representing the highest possible product quality. . Provided that we collect enough production line data, machine learning offers a rich toolbox for predicting the expected QA verb!QA!QA score or whether our QA verb!QA!QA score will lie above a certain threshold. The former application is called regression (predict QA verb!QA!QA value) while the latter is called classification (predict if QA verb!QA!QA is greater than a threshold). . To implement these applications there are numerous established machine learning models, some of which are: . Linear and polynomial regression, | (Deep) neural networks / deep learning, | Classification and regression trees (CART), | Ensemble methods such as random forests and gradient tree boosting, | Logistic regression, | Support vector machines, | Gaussian processes, and | Naive Bayes. | . These machine learning models allow us to learn the potentially complex relationship between our production line parameters (AAA, BBB, CCC, and DDD) and resultant product quality (QA verb!QA!QA). . These are also the same machine learning models used routinely to implement the other aforementioned industry applications such as demand forecasting or churn prediction. . So training a machine learning model on our production line data we get something like this: . QA=model(A,B,C,D) verb!QA! = verb!model!(A, B, C, D)QA=model(A,B,C,D). . Where our model predicts the quality of our final product based on the parameters of our production line. . Using our prediction model . Imagine we manage to train a highly accurate machine learning model on our production line data - what do we do with it? . Sure, if measuring the quality of our product (QA verb!QA!QA) is prohibitively expensive we could cut cost by using our model to determine the quality score for items coming off our production line. . We could also try and adapt our model so that it provides accurate quality score predictions early in the production cycle for a given item - thus allowing us to cancel work on substandard items before they consume resources unnecessarily. . But really, what we are most likely interested in is: . How should we tweak individual production parameters to increase product quality? . Issue with our prediction model . Unfortunately, our machine learning model does not hold the answer to our core question: Out of the box it will not help us optimize individual production parameters to improve our quality score QA verb!QA!QA. . Let’s see why that is. . We can write our machine learning model as a function: . QA=f(A,B,C,D), verb!QA! = f(A, B, C, D),QA=f(A,B,C,D), . where fff is our model that predicts the quality score for items manufactured with production parameters AAA, BBB, CCC, and DDD. . The issue with this model is that it learns the relationship between quality score and production parameters “in bulk” - or in other words the conditional distribution of QA verb!QA!QA jointly on production parameters AAA, BBB, CCC, and DDD. . The only sensible operation we can carry out with this model is asking all production line workers for their presently (or planned) production parameter values, inputting them into our model, and informing our colleagues in production of the quality score we expect to achieve with their configuration. . Of course, a far more desirable operation would be to take the presently used production parameters, e.g. . (A,B,C,D)=(a,b,c,d),(A, B, C, D) = (a, b, c, d),(A,B,C,D)=(a,b,c,d), . tweak a single parameter, e.g. increase parameter BBB by 10%, and compute the newly expected quality score . QA modified=f(a,1.1×b,c,d). verb!QA modified! = f(a, 1.1 times b, c, d).QA modified=f(a,1.1×b,c,d). . If the newly predicted QA modified verb!QA modified!QA modified turns out greater than the presently achieved or predicted quality score we would go back to the production floor and ask our colleague manning the workstation to increase their parameter BBB by 10%. . Unfortunately, this operation of tweaking and optimizing individual input variables (production parameters) is not supported by our machine learning model. . In essence, our model learns the bulk relationship between the entire set of production parameters and quality score, and not the individual impact of each production parameter on the quality score - as illustrated in the following sketch: . . This problem pertains to most machine learning models independent of the specific problem tackled. . How do we modify our approach so that we can estimate the impact of individual production parameter modifications and help our colleagues on the production floor make optimal, surgical changes to their manufacturing process? . One major piece to closing this gap is causality. . The missing piece: causality . Variables and parameters are often interconnected . To disentangle their individual contributions, we first need to account for how our production parameters influence one another and the quality score. . To understand the connections between our production parameters and the quality score, we pop down to the production floor and chat with our colleagues manning the individual workstations. From them we learn: . When the stiffness of the raw material (A) increases then downstream workers increase the pressure applied on the item (B), | Depending on the stiffness of the raw material (A) the type of downstream drill (C) is chosen, | The worker choosing the type of drill (C) also informs the worker moulding our item under pressure (B) since they need to balance bore friction and metal stiffness, | The worker applying a specific type of finish (D) to our product is not affected by the choices of their co-workers. | . Armed with these insights from our production floor we draw up the following causal model - showing the connections among our production parameters and the quality score of our product: . . A simplistic, and perhaps crude, way of interpreting the arrows between parameters AAA, BBB, and CCC is to imagine these arrows were unidirectional mechanical springs. If I pull on parameter AAA by changing it somehow, I also move around parameters BBB and CCC - however pulling on CCC does not affect AAA as that spring is unidirectional. . Interconnected variables are hard to disentangle . To reiterate, our main use for a machine learning model that predicts the quality score of manufactured items is to support our colleagues on the production floor in optimizing individual production parameters to maximize our overall output quality. . Our causal model (Figure 3) makes it apparent that our model should be more complex than previously thought. The connections in our causal model indicate that we should write our model as . QA=f(g(A,h(A)),h(A),D), verb!QA! = f(g(A, h(A)), h(A), D),QA=f(g(A,h(A)),h(A),D), . where both BBB and CCC are functions of AAA . B=g(A,C),C=h(A)B = g(A, C), C = h(A)B=g(A,C),C=h(A). . In short: In addition to teaching our machine learning model the relationship between our four production parameters and the quality score, we also need to teach our model the various relationships between our production parameters before we can use our model to optimize individual workstations. . To learn all these relationships we need a lot of data and variation in production parameters that show how the parameters influence one another. We may be inclined to go down to the manufacturing floor and start experimenting at random with our production parameters generating enough data to learn both the interconnections between the production parameters (B=g(A,C)B = g(A, C)B=g(A,C), C=h(A)C = h(A)C=h(A)) and their individual impact on our quality score QA verb!QA!QA. . However, each manufactured item with substandard quality due to a randomly selected combination of production parameters means money lost for our company. . This approach, called randomized experiment or randomized controlled trial, is oftentimes undesirable or simply infeasible: . In the case of our production line (and smart manufacturing in general), randomly assigning production parameters to measure their individual impact would be both very time consuming and prohibitively expensive - imagine all the low-quality items made and discarded, | In e-commerce, for smart pricing, it is in fact prohibited by law to price the same item differently for different customers in the same market, | In customer care it would be counterproductive to treat some customers poorly just to see how their purchase behavior and brand loyalty are affected, | In predictive maintenance it would be questionable or prohibited not to perform best practice maintenance steps on individual items to see how they deteriorate differently, and | In online marketing it is ofentimes prohibitively complex or infeasible, due to data protection legislation, to cleanly understand the effect (uplift) of isolated marketing parameters on individual users. | Hence, we cannot rely on randomized experiments to understand how to optimally help our production floor colleagues. . So what we are left with is continue doing what we do on the production floor, and use the production data generated under normal operating conditions (observational data) to estimate the impact of each production parameter individually on our quality score. . To achieve this we need to rely on both our causal model (Figure 3) and a toolset from the field of causal inference called do-calculus. . Causal inference toolset: Do-calculus . The components we have gathered so far are: . A causal model of our production floor (Figure 3), and | Lots of observational data of production parameter settings and measured resultant quality score QA verb!QA!QA. | . Let us go through a thought experiment illustrated in the image below (Figure 4): How would the expected quality score QA verb!QA!QA change if our colleague in workstation BBB chose their production parameter independently of their colleagues in workstations AAA and CCC? . . This is what the notation in Figure 4 relates to: do(B=b)do(B = b)do(B=b) describes a theoretical scenario where production parameter BBB is assigned to hold value bbb - instead of a scenario where parameter BBB is observed to equal value bbb. . Note the seemingly subtle but important difference: If we go back to our observational data generated under normal operating conditions then observing B=bB = bB=b implies something about production parameters AAA and CCC since, according to our causal model (Figure 3), these are causally connected! However, forcing (or intervening on) parameter BBB by setting it to value bbb breaks that causal link between the workstations - and this is what the notation do(B=b) verb!do!(B=b)do(B=b) signifies. . In essence do(B=b) verb!do!(B=b)do(B=b) would mean asking our colleague in workstation BBB to stop listening to their colleagues in workstations AAA and CCC. . Since doing so in reality on the production floor would incur high cost due to lost production we simulate the intervention do(B=b) verb!do!(B = b)do(B=b)! using the observational data we collect under normal operating conditions anyway. . To simulate do(B=b) verb!do!(B = b)do(B=b) we apply a set of algebraic rules, called do-calculus, to our causal model from Figure 3 and draw on our regular observational data from the production floor. . In technical terms, do-calculus allows us to turn theoretical interventional distributions into regular conditional distributions that we can compute using our observational data. . With this toolset we can finally estimate the individual effect of tweaking production parameter BBB on quality score QA verb!QA!QA thus allowing us to optimize surgically our manufacturing output. . Becoming a causality-driven company . I hope I managed to convince you of the important difference between recording lots of data and being able to optimize individual steps of your business operations based on your data. . The former is limited to prediction: Your organization can predict future outcomes if all internal processes continue to be operated as usual. This is the essence of the data-driven company. . The latter adds explanation to prediction: Your organization can explain how outcomes and future outcomes come about. And once you can explain how your processes lead to outcomes you can start optimizing individual components of your processes and organization. This enables you to analyze what would happen if you changed “the way you have always done things” - thus elevating you to a causality-driven company. . How can you start using causal inference and tools such as do-calculus to optimize your business? And how do you eventually become a causality-driven company? . A roadmap of this kind, a causality strategy if you will, would certainly be an interesting subject for another blog article. . Until then, I would start by becoming a data-driven company first: . Set up data collection across all key operations of your business, | Start working on machine learning prototypes for predicting key business metrics based on your collected data, and | Start asking yourself: Once I can predict my business metrics with machine learning, what specific changes should I make to my business to optimize these metrics? | . Once you get to the point of wanting to make surgical decisions based on your machine learning model, causality and causal inference will start playing key roles. . .",
            "url": "https://georg.io/the_causality-driven_company",
            "relUrl": "/the_causality-driven_company",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Decision intelligence from historical observations for optimal marketing resource use",
            "content": "Summary . Marketing is a key success and revenue driver in B2C markets: An appropriate message placed at the appropriate time with a prospective customer will increase your business success. . However, marketing is also a major cost driver for businesses: Marketing efforts that are too broad, target the wrong audience, or convey the wrong message waste resources. . In the case of direct marketing via phone conversations a key cost factor is the amount of time a sales call agent spends with the prospective customer on the phone. . In this article we explore, rudimentarily, direct marketing data of a Portuguese financial institution. . We explore the relationship between call duration and success (purchase of offered financial product), and show that consideration of customer-specific factors influences how you should allocate your marketing resources. . Our prototypical analysis can be usueful in devising data-driven marketing and sales strategies that offer decision intelligence for your call agents. . Fetch the data . For our prototype we use the openly accessible Bank Marketing Data Set from the UC Irvine Machine Learning Repository. . !wget --quiet https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip . !unzip -oqq bank.zip . Load Python libraries . import graphviz import numpy as np import pandas as pd import seaborn as sns from sklearn.cluster import KMeans from sklearn.preprocessing import LabelEncoder . np.random.seed(42) . Prepare data . The data we work with here contain a number of categorical and numerical variables. To keep our analysis and prototype simple we will focus on only a handful and remove the remainder. . #collapse df = pd.read_csv(&#39;bank.csv&#39;, delimiter=&#39;;&#39;) df[&#39;success&#39;] = df[&#39;y&#39;] del df[&#39;y&#39;] df[&#39;success&#39;] = df[&#39;success&#39;].replace(&#39;no&#39;, 0) df[&#39;success&#39;] = df[&#39;success&#39;].replace(&#39;yes&#39;, 1) del df[&#39;education&#39;] del df[&#39;default&#39;] del df[&#39;housing&#39;] del df[&#39;loan&#39;] del df[&#39;contact&#39;] del df[&#39;day&#39;] del df[&#39;month&#39;] del df[&#39;campaign&#39;] del df[&#39;pdays&#39;] del df[&#39;previous&#39;] del df[&#39;poutcome&#39;] . . Our tabular data set now looks as follows: Each prospective (and in some cases eventual) customer whom a call agent conversed with fills a row. On each row we have numerical variables (age, account balance, duration of sales interaction) and categorical variables (job / employment status and marital status). Our data set contains 4,521 sales interactions. . df . age job marital balance duration success . 0 30 | unemployed | married | 1787 | 79 | 0 | . 1 33 | services | married | 4789 | 220 | 0 | . 2 35 | management | single | 1350 | 185 | 0 | . 3 30 | management | married | 1476 | 199 | 0 | . 4 59 | blue-collar | married | 0 | 226 | 0 | . ... ... | ... | ... | ... | ... | ... | . 4516 33 | services | married | -333 | 329 | 0 | . 4517 57 | self-employed | married | -3313 | 153 | 0 | . 4518 57 | technician | married | 295 | 151 | 0 | . 4519 28 | blue-collar | married | 1137 | 129 | 0 | . 4520 44 | entrepreneur | single | 1136 | 345 | 0 | . 4521 rows × 6 columns . High-level model: more is better . A blanket approach to marketing and sales may be: More resources lead to greater success. . So in the case of direct marketing on the phone we could expect that the more time we spend with a prospective customer on the phone, the bigger our success rate. . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G duration duration success success duration&#45;&gt;success To test our model, we discretize the duration of our interaction with the customer into six duration buckets: bucket 1 holds the shortest interactions while bucket 6 holds the longest interactions. . no_buckets = 6 df[&#39;duration_bucket&#39;] = pd.qcut(df[&#39;duration&#39;], no_buckets, labels=[f&#39;bucket {b + 1}&#39; for b in range(no_buckets)]) . df.groupby(&#39;duration_bucket&#39;).agg({&#39;success&#39;: &#39;mean&#39;}) . success . duration_bucket . bucket 1 0.003932 | . bucket 2 0.031662 | . bucket 3 0.048193 | . bucket 4 0.095174 | . bucket 5 0.155378 | . bucket 6 0.358090 | . Looking at the average success rate in each duration bucket shows us that there is positive correlation between the duration of a sales interaction and our success rate - just as our model predicted. . Hence, more marketing spend appears to lead to greater success in general. . From a data perspective this is a pretty disappointing result as we expect to glean more intelligent insights from all the data we collected. . Nuanced model: more isn&#39;t always better and there are always tradeoffs . Let&#39;s dig deeper into what is going on here: Yes, the duration of the interaction between call agent and prospective customer likely influences our success rate. . However, call agents also probably choose to spend more time on the phone with customers whose account balance is higher - hoping for a greater chance of a sale. That same account balance also likely influences how affine the customer is for spending more money on financial products. . Both present job status and marital status are also likely candidates for influencing an affinity for financial products. . And age of the customer probably influences both their job and marital status. . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G age age job job age&#45;&gt;job marital marital age&#45;&gt;marital balance balance job&#45;&gt;balance success success job&#45;&gt;success marital&#45;&gt;success duration duration balance&#45;&gt;duration balance&#45;&gt;success duration&#45;&gt;success Since a customer&#39;s account balance probably influences both how much time we spend with them on the phone and their likelihood of purchasing another financial product we will control for account balance. . We control for account balance by training a cluster algorithm that segments our data set into three groups of similar account balance. . df[&#39;job&#39;] = LabelEncoder().fit_transform(df[&#39;job&#39;]) df[&#39;marital&#39;] = LabelEncoder().fit_transform(df[&#39;marital&#39;]) . segmenter = KMeans(n_clusters=3, random_state=42) . df[&#39;segment&#39;] = segmenter.fit_predict(df[[&#39;balance&#39;]]) . df[&#39;segment&#39;] = df[&#39;segment&#39;].replace({0: &#39;low balance&#39;, 1: &#39;high balance&#39;, 2: &#39;medium balance&#39;}) . Looking at both the average account balance and age in our three segments, we notice that our clustering algorithm picked out low, medium, and high balance segments. . We also notice that average age correlates with average balance in these three segments hence our intuition codified in our above model seems valid. . df.groupby(&#39;segment&#39;).agg({&#39;age&#39;: &#39;mean&#39;, &#39;balance&#39;: &#39;mean&#39;}) . age balance . segment . high balance 44.542857 | 18361.771429 | . low balance 40.845577 | 543.930678 | . medium balance 42.911111 | 5202.864957 | . Now, what about the effectiveness of our marketing resources in each segment? . Visualizing our rate of success in the six duration buckets broken down by account balance segment we see a more nuanced picture: . Customers with low account balances really need to be worked on and only show success rates greater than 20% in the highest duration bucket 6, | customers with medium balances already show a greater than 20% purchase likelihood in duration bucket 4, and | customers with high balances actually max out in duration bucket 5 and drop below a 20% success rate in bucket 6. | . success_rates = df.groupby([&#39;segment&#39;, &#39;duration_bucket&#39;]).agg({&#39;success&#39;: &#39;mean&#39;}).reset_index() . sns.set(rc={&#39;figure.figsize&#39;: (10,6)}) sns.barplot( x=&#39;duration_bucket&#39;, y=&#39;success&#39;, hue=&#39;segment&#39;, data=success_rates, hue_order=[&#39;low balance&#39;, &#39;medium balance&#39;, &#39;high balance&#39;] ); . Our more nuanced model and analysis provide us with data-driven insights that provide actionable and testable advice: . We should probably re-evaluate whether low balance individuals are sensible targets for our marketing campaigns given how resource-intensive they are, | compute the profit and loss tradeoff between spending bucket 4 and bucket 6 resources on medium balance individuals, and | ensure that we do not overdo it with our calls for high balance individuals. | .",
            "url": "https://georg.io/2020/01/12/Optimal_resource_allocation_and_uplift_for_direct_marketing",
            "relUrl": "/2020/01/12/Optimal_resource_allocation_and_uplift_for_direct_marketing",
            "date": " • Jan 12, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Smart maintenance based on vehicle CAN bus data from scratch in Python",
            "content": "Summary . Equipment that behaves anomalously or breaks down unexpectedly is a major cost driver in manufacturing, logistics, public transport, and any other sector that relies on complex machinery. . A big promise of data analytics and machine learning in this space is to detect anomalies in machinery automatically and to alert their user of occurring faults. As an extension, the prediction of machinery faults and breakdowns is an important field of application. . Automated detection and prediction of machinery breakdown is a key algorithmic approach behind smart and predictive maintenance. . In this article we showcase a simple algorithmic approach for anomaly detection in the space of automated engine health detection. . Our approach here can be an interesting starting point for the development of smart telematics solutions for automated and predictive vehicle breakdown detection. . Fetch the data . We&#39;ll make use of an open data set of vehicle CAN bus data, called Automotive CAN bus data: An Example Dataset from the AEGIS Big Data Project. . A CAN bus is a local network of sensors and actuators in modern vehicles that provides a stream of data for all important signals of a vehicle - such as its present velocity, interior temperature, and potentially hundreds of other signals. . This data set encompasses time series data (traces) of various vehicles driven by different drivers. . Let&#39;s go ahead and download a data set for driver 1 and a data set for driver 2: . !wget --quiet https://zenodo.org/record/3267184/files/20181113_Driver1_Trip1.hdf . !wget --quiet https://zenodo.org/record/3267184/files/20181114_Driver2_Trip3.hdf . Load libraries . Here we import all necessary Python libraries for our analysis and algorithm: . import h5py from matplotlib import pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.mixture import GaussianMixture . plt.rcParams[&#39;figure.figsize&#39;] = (10,10) sns.set(style=&quot;darkgrid&quot;) . Load vehicle data . Let&#39;s load the data for driver 1 and driver 2 into memory: . driver_1 = h5py.File(&#39;20181113_Driver1_Trip1.hdf&#39;, &#39;r&#39;) driver_2 = h5py.File(&#39;20181114_Driver2_Trip3.hdf&#39;, &#39;r&#39;) . Both files contain multiple subgroups of data, one of which is the aformentioned CAN bus: . list(driver_1.keys()) . [&#39;AI&#39;, &#39;CAN&#39;, &#39;GPS&#39;, &#39;Math&#39;, &#39;Plugins&#39;] . list(driver_2.keys()) . [&#39;AI&#39;, &#39;CAN&#39;, &#39;GPS&#39;, &#39;Math&#39;, &#39;Plugins&#39;] . Turn time series data into tables . The CAN bus data comes in serialized form - written out in series in a nested format. . To handle the CAN bus data more efficiently we&#39;ll turn it into tables that are easier to inspect and handle. . data_driver_1 = {} data_driver_2 = {} for channel_name, channel_data in driver_1[&#39;CAN&#39;].items(): data_driver_1[channel_name] = channel_data[:, 0] table_driver_1 = pd.DataFrame( data=data_driver_1, index=channel_data[:, 1] ) table_driver_1 = table_driver_1.loc[:, table_driver_1.nunique() &gt; 1] for channel_name, channel_data in driver_2[&#39;CAN&#39;].items(): data_driver_2[channel_name] = channel_data[:, 0] table_driver_2 = pd.DataFrame( data=data_driver_2, index=channel_data[:, 1] ) table_driver_2 = table_driver_2.loc[:, table_driver_2.nunique() &gt; 1] . The tabular data for driver 1 looks as follows - it holds 158,659 measured time points in 28 channels that we deem relevant: . table_driver_1 . AccPedal AirIntakeTemperature AmbientTemperature BoostPressure BrkVoltage ENG_Trq_DMD ENG_Trq_ZWR ENG_Trq_m_ex EngineSpeed_CAN EngineTemperature Engine_02_BZ Engine_02_CHK OilTemperature1 SCS_01_BZ SCS_01_CHK SCS_Cancel SCS_Tip_Down SCS_Tip_Set SCS_Tip_Up SteerAngle1 Trq_FrictionLoss Trq_Indicated VehicleSpeed WheelSpeed_FL WheelSpeed_FR WheelSpeed_RL WheelSpeed_RR Yawrate1 . 0.000000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 809.500000 | 93.0 | 6.000000 | 168.000000 | 82.0 | 9.000000 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.190000 | . 0.050000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 809.500000 | 93.0 | 6.000000 | 168.000000 | 82.0 | 9.000000 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.190000 | . 0.100000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 810.215759 | 93.0 | 10.331579 | 163.663162 | 82.0 | 9.333000 | 26.000999 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.209900 | . 0.150000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.384237 | 807.657654 | 93.0 | 9.605911 | 165.674881 | 82.0 | 9.833000 | 24.500999 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.384237 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.204887 | . 0.200000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 805.500000 | 93.0 | 4.358586 | 172.641418 | 82.0 | 10.333000 | 24.333000 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 28.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.200000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 7932.700195 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 20.0 | 19.0 | 29.000000 | 797.500000 | 96.0 | 8.174129 | 84.825874 | 96.0 | 10.493239 | 24.493240 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 29.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.130000 | . 7932.750000 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 800.500000 | 96.0 | 13.164251 | 145.835754 | 96.0 | 10.993991 | 24.993992 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 29.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.123609 | . 7932.799805 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 797.790588 | 96.0 | 2.177665 | 156.822342 | 96.0 | 11.494000 | 27.469999 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 28.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.178596 | . 7932.850098 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 796.000000 | 96.0 | 7.180095 | 153.739334 | 96.0 | 11.994000 | 29.969999 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 28.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.157268 | . 7932.899902 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 794.500000 | 96.0 | 12.155440 | 146.844559 | 96.0 | 12.494247 | 30.494247 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 28.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140000 | . 158659 rows × 28 columns . The tabular data for driver 2 looks as follows - it holds 136,154 measured time points in 29 channels that we deem relevant: . table_driver_2 . AccPedal AirIntakeTemperature AmbientTemperature BoostPressure BrkVoltage ENG_Trq_DMD ENG_Trq_ZWR ENG_Trq_m_ex EngineSpeed_CAN EngineTemperature Engine_02_BZ Engine_02_CHK OilTemperature1 SCS_01_BZ SCS_01_CHK SCS_Cancel SCS_Tip_Down SCS_Tip_Restart SCS_Tip_Set SCS_Tip_Up SteerAngle1 Trq_FrictionLoss Trq_Indicated VehicleSpeed WheelSpeed_FL WheelSpeed_FR WheelSpeed_RL WheelSpeed_RR Yawrate1 . 0.000000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 20.000000 | 21.0 | 30.000000 | 791.000000 | 96.0 | 6.000000 | 59.000000 | 94.0 | 10.000000 | 24.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140000 | . 0.050000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 30.000000 | 791.000000 | 96.0 | 10.688680 | 102.688683 | 94.0 | 10.000000 | 24.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.120000 | . 0.100000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 30.000000 | 791.165039 | 96.0 | 4.381188 | 105.079208 | 94.0 | 10.110166 | 24.110165 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.148120 | . 0.150000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 30.751268 | 787.500000 | 96.0 | 4.725888 | 104.725891 | 94.0 | 10.610916 | 24.610916 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.751268 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.126384 | . 0.200000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 31.000000 | 791.000000 | 96.0 | 9.733668 | 101.733665 | 94.0 | 11.111500 | 25.557501 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 31.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.135592 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6807.450195 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 21.000000 | 21.0 | 31.000000 | 815.500000 | 94.5 | 10.693069 | 118.693069 | 95.0 | 5.992500 | 22.394501 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.400002 | 30.0 | 31.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.130326 | . 6807.500000 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 21.000000 | 21.0 | 30.682692 | 813.500000 | 94.5 | 5.192307 | 120.884613 | 95.0 | 0.100500 | 18.100500 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.400002 | 30.0 | 30.682692 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.150000 | . 6807.549805 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 21.000000 | 21.0 | 31.000000 | 816.000000 | 94.5 | 4.676768 | 120.676765 | 95.0 | 0.600500 | 18.600500 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.476120 | 30.0 | 31.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140351 | . 6807.600098 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 20.323383 | 21.0 | 30.000000 | 819.587524 | 94.5 | 9.676617 | 74.726372 | 95.0 | 1.100550 | 18.698349 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.476616 | 30.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140000 | . 6807.649902 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 22.000000 | 21.0 | 30.722773 | 810.048096 | 94.5 | 14.693069 | 178.693069 | 95.0 | 1.600800 | 17.197599 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.500000 | 31.0 | 30.722773 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.129625 | . 136154 rows × 29 columns . Monitoring engine health . One use case of automated anomaly detection lies in checking the health status of a vehicle&#39;s engine. . Here we&#39;ll look at engine oil temperature as a function of velocity: As you&#39;ll notice in the below plots, oil temperature goes up with higher velocity and increased operating duration. . Engine 1 - healthy . Let&#39;s look at the engine of the vehicle of driver 1 where engine oil temperature appears normal as it keeps to within a certain band: . temperature_1 = table_driver_1[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]].copy() sns.relplot( x=&#39;index&#39;, y=&#39;value&#39;, hue=&#39;channel&#39;, kind=&#39;line&#39;, data=temperature_1.reset_index().melt(id_vars=&#39;index&#39;, var_name=&#39;channel&#39;) ); . Plotting engine oil temperature against vehicle velocity makes the correlation between the two metrics more apparent: . plot = sns.scatterplot( x=&#39;VehicleSpeed&#39;, y=&#39;OilTemperature1&#39;, data=temperature_1, color=&#39;blue&#39;, alpha=.1 ); plot.axes.set_ylim(0., 110.); . Engine 2 - unhealthy . Let&#39;s look at velocity and engine oil temperature for vehicle 2 (where I deliberately introduce an anomaly between 5000 and 5500 seconds of the trace). . You&#39;ll notice a spike in engine oil temperature which indicates unhealthy behavior of the vehicle&#39;s engine: . temperature_2 = table_driver_2[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]].copy() temperature_2.loc[5000:5500, &#39;OilTemperature1&#39;] *= 1.15 sns.relplot( x=&#39;index&#39;, y=&#39;value&#39;, hue=&#39;channel&#39;, kind=&#39;line&#39;, data=temperature_2.reset_index().melt(id_vars=&#39;index&#39;, var_name=&#39;channel&#39;) ); . plot = sns.scatterplot( x=&#39;VehicleSpeed&#39;, y=&#39;OilTemperature1&#39;, data=temperature_2, color=&#39;blue&#39;, alpha=.1 ); plot.axes.set_ylim(0., 140.); . Anomaly detection algorithm: learn what observations to expect to recognize anomalies . Let&#39;s look at the visual distribution of engine oil temperature and velocity again and directly compare vehicle 1 with vehicle 2. . temperature_1[&#39;vehicle&#39;] = &#39;vehicle 1&#39; temperature_2[&#39;vehicle&#39;] = &#39;vehicle 2&#39; combined = pd.concat([temperature_1, temperature_2], axis=0, sort=True) plot = sns.scatterplot( x=&#39;VehicleSpeed&#39;, y=&#39;OilTemperature1&#39;, data=combined, hue=&#39;vehicle&#39;, alpha=.1 ); plot.axes.set_ylim(0., 140.); . You&#39;ll notice that the engine oil temperature of vehicle 2 tends to be higher than of vehicle 1 and vehicle 2 shows an island of high engine oil temperature separated from the bulk of data points. . We can use this by modelling the distribution of value pairs velocity and oil temperature that we would usually expect to observe. . Let&#39;s model the expected distribution of data points with a model called Gaussian mixture models. This model fits a set of Gaussian distributions to the distribution of value pairs we observed for vehicle 1 - thus defining what a healthy distribution of values looks like. . model = GaussianMixture(n_components=4) . model.fit(temperature_1[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]]) . GaussianMixture(covariance_type=&#39;full&#39;, init_params=&#39;kmeans&#39;, max_iter=100, means_init=None, n_components=4, n_init=1, precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weights_init=None) . After training our model on the observations for vehicle 1, let&#39;s score the likelihood of observing each observation we have on vehicle 2: . temperature_2[&#39;health_score&#39;] = model.score_samples(temperature_2[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]]) . sns.relplot( x=&#39;index&#39;, y=&#39;value&#39;, hue=&#39;metric&#39;, kind=&#39;line&#39;, data=temperature_2[[&#39;OilTemperature1&#39;, &#39;health_score&#39;]].reset_index().melt(id_vars=&#39;index&#39;, var_name=&#39;metric&#39;) ); . To clarify what we did here: We learned what a healthy distribution of vehicle velocity and engine oil temperature looks like based on data from vehicle 1 and applied this model to data from vehicle 2. . Looking at the health score we compute for vehicle 2 we notice that our health score drops markdely between 5000 and 5500 seconds into our CAN bus trace - exactly where oil temperature spikes unhealthily. . While we would need to do a lot more validation and model calibration before we could use this approach in a live environment, we can already see the potential of this approach. . With this approach we can start devising data-driven products and services for smart telematics and smart maintenance applications. .",
            "url": "https://georg.io/2020/01/05/CAN_bus_anomaly_detection_gaussian_mixture_model",
            "relUrl": "/2020/01/05/CAN_bus_anomaly_detection_gaussian_mixture_model",
            "date": " • Jan 5, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Multiclass classification applied to a stream of documents in Python",
            "content": "Classifying Wikipedia Changes . I recently joined a Kaggle competition on multilabel text classification and have learned a ton from basic code that one of the competitors shared in the forums. . The code of the genereous competitor does logistic regression classification for multiple classes with stochastic gradient ascent. It is further well-suited for online learning as it uses the hashing trick to one-hot encode boolean, string, and categorial features. . To better understand these methods and tricks I here apply some of them to a multilabel problem I chose mostly for the easy access to a constant stream of training data: . All recent changes on Wikipedia are tracked on this special page where we can see a number of interesting features such as the length of the change, the contributor&#39;s username, the title of the changed article, and the contributor&#39;s comment for a given change. . Using the Wikipedia API to look at this stream of changes we can also see how contributors classify their changes as bot, minor, and new. Multiple label assignments are possible, so that one contribution may be classified as both bot and new. . Here I will listen to this stream of changes, extract four features (length of change, comment string, username, and article title), and train three logistic regression classifiers (one for each class) to predict the likelihood of a change belonging to each one of them. The training is done with the stochastic gradient ascent method. . One caveat: I am a complete novice when it comes to most of this stuff so please take everything that follows with a grain of salt - on the same note I would be forever grateful for any feedback especially of the critical kind so that I can learn and improve. . %matplotlib inline import matplotlib from matplotlib import pyplot as pt import requests import json from math import log, exp, sqrt from datetime import datetime, timedelta import itertools from collections import defaultdict . The API that Wikipedia offer to listen to the stream of recent changes is described here. . URL = (&#39;http://en.wikipedia.org/w/api.php?format=json&amp;action=query&amp;list=recentchanges&amp;rcprop=parsedcomment&#39; &#39;%7Ctimestamp%7Ctitle%7Cflags%7Cids%7Csizes%7Cflags%7Cuser&amp;rclimit=100&#39;) . The logistic regression classifier requires us to compute the dot product between a feature vector $ mathbf{x}$ and a weight vector $ mathbf{w}$. . $$ mathbf{w}^ text{T} mathbf{x} = w_0 x_0 + w_1 x_1 + w_2 x_2 + ldots + w_N x_N.$$ . As by convention, the bias of the model is encoded with feature $x_0 = 1$ for all observations - the only thing that will change about the $w_0 x_0$-term is weight $w_0$ upon training. The length of the article change is tracked with numerical feature $x_1$ which equals the number of character changes (hence $x_1$ is either positive or negative for text addition and removal respectively). . As in the Kaggle code that our code is mostly based upon, string features are one-hot encoded using the hashing trick: . The string features extracted for each observed article change are username, a parse of the comment, and the title of article. Since this is an online learning problem there is no way of knowing how many unique usernames, comment strings, and article titles are going to be observed. . With the hashing trick we decide ab initio that D_sparse-many unique values across these three features are sufficient to care about: Our one-hot encoded feature space has dimension D_sparse and can be represented as a D_sparse-dimensional vector filled with 0&#39;s and 1&#39;s (feature not present / present respectively). . The hash in hashing trick comes from the fact that we use a hash function to convert strings to integers. Suppose now that we chose D_sparse = 3 and our hash function produces hash(&quot;georg&quot;) = 0, hash(&quot;georgwalther&quot;) = 2, and hash(&quot;walther&quot;) = 3 for three observed usernames. . For username georg we get feature vector $[1, 0, 0]$ and for username georgwalther we get $[0, 0, 1]$. The hash function maps username walther outside our 3-dimensional feature space and to close this loop we not only use the hash function but also the modulus (which defines an equivalence relation?): . hash(&quot;georg&quot;) % D_sparse = 0, hash(&quot;georgwalther&quot;) % D_sparse = 2, and hash(&quot;walther&quot;) % D_sparse = 0 . This illustrates one downside of using the hashing trick since we will now map usernames georg and walther to the same feature vector $[1, 0, 0]$. We are therefore best adviced to choose a big D_sparse to avoid mapping different feature values to the same one-hot-encoded feature - but probably not too big to preserve memory. . For each article change observation we only map three string features into this D_sparse-dimensional one-hot-encoded feature space - out of D_sparse-many vector elements there will only ever be three ones among (D_sparse-3) zeros (if we do not map to the same vector index multiple times). We will therefore use sparse encoding for these feature vectors (hence the sparse in D_sparse). . We will also normalize the length of change on the fly using an online algorithm for mean and variance estimation. . D = 2 # number of non-sparse features D_sparse = 2**18 # number of sparsely-encoded features . def get_length_statistics(length, n, mean, M2): &quot;&quot;&quot; https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Incremental_algorithm &quot;&quot;&quot; n += 1 delta = length-mean mean += float(delta)/n M2 += delta*(length - mean) if n &lt; 2: return mean, 0., M2 variance = float(M2)/(n - 1) std = sqrt(variance) return mean, std, M2 def get_data(): X = [1., 0.] # bias term, length of edit X_sparse = [0, 0, 0] # hash of comment, hash of username, hash of title Y = [0, 0, 0] # bot, minor, new length_n = 0 length_mean = 0. length_M2 = 0. while True: r = requests.get(URL) r_json = json.loads(r.text)[&#39;query&#39;][&#39;recentchanges&#39;] for el in r_json: length = abs(el[&#39;newlen&#39;] - el[&#39;oldlen&#39;]) length_n += 1 length_mean, length_std, length_M2 = get_length_statistics(length, length_n, length_mean, length_M2) X[1] = (length - length_mean)/length_std if length_std &gt; 0. else length X_sparse[0] = abs(hash(&#39;comment_&#39; + el[&#39;parsedcomment&#39;])) % D_sparse X_sparse[1] = abs(hash(&#39;username_&#39; + el[&#39;user&#39;])) % D_sparse X_sparse[2] = abs(hash(&#39;title_&#39; + el[&#39;title&#39;])) % D_sparse Y[0] = 0 if el.get(&#39;bot&#39;) is None else 1 Y[1] = 0 if el.get(&#39;minor&#39;) is None else 1 Y[2] = 0 if el.get(&#39;new&#39;) is None else 1 yield Y, X, X_sparse . def predict(w, w_sparse, x, x_sparse): &quot;&quot;&quot; P(y = 1 | (x, x_sparse), (w, w_sparse)) &quot;&quot;&quot; wTx = 0. for i, val in enumerate(x): wTx += w[i] * val for i in x_sparse: wTx += w_sparse[i] # *1 if i in x_sparse try: wTx = min(max(wTx, -100.), 100.) res = 1./(1. + exp(-wTx)) except OverflowError: print wTx raise return res . def update(alpha, w, w_sparse, x, x_sparse, p, y): for i, val in enumerate(x): w[i] += (y - p) * alpha * val for i in x_sparse: w_sparse[i] += (y - p) * alpha # * feature[i] but feature[i] == 1 if i in x . K = 3 w = [[0.] * D for k in range(K)] w_sparse = [[0.] * D_sparse for k in range(K)] predictions = [0.] * K alpha = .1 . time0 = datetime.now() training_time = timedelta(minutes=10) ctr = 0 for y, x, x_sparse in get_data(): for k in range(K): p = predict(w[k], w_sparse[k], x, x_sparse) predictions[k] = float(p) update(alpha, w[k], w_sparse[k], x, x_sparse, p, y[k]) ctr += 1 # if ctr % 10000 == 0: # print &#39;samples seen&#39;, ctr # print &#39;sample&#39;, y # print &#39;predicted&#39;, predictions # print &#39;&#39; if (datetime.now() - time0) &gt; training_time: break . ctr . As we can see, we crunched through 106,401 article changes during our ten-minute online training. . It would be fairly hard to understand the link between the D_sparse-dimensional one-hot-encoded feature space and the observed / predicted classes. However we can still look at the influence that the length of the article change has on our classification problem . print w[0] print w[1] print w[2] . Here we can see that the weight of the length of change for class 0 (bot) is -1.12, for class 1 (minor) is -0.97, and for class 2 (new) is 2.11. . Intuitively this makes sense since many added characters (big positive change) should make classification as a minor change less likely and classification as a new article more likely: For an observed positive character count change $C$, $2.11 C$ will place us further to the right, and $-0.97 C$ further to the left along the $x$-axis of the sigmoid function: . (from http://gaelvaroquaux.github.io/scikit-learn-tutorial/supervised_learning.html#classification) . Further below we will see that the vast majority of bot changes are classified as minor changes hence we would expect to see correlation between the weights of this feature for these two classes. . To further evaluate our three classifiers, we observe another 10,000 Wikipedia changes and construct a confusion matrix. . no_test = 10000 test_ctr = 0 classes = {c: c_i for c_i, c in enumerate(list(itertools.product([0, 1], repeat=3)))} confusion_matrix = [[0 for j in range(len(classes))] for i in range(len(classes))] predicted = [0, 0, 0] for y, x, x_sparse in get_data(): for k in range(K): p = predict(w[k], w_sparse[k], x, x_sparse) predicted[k] = 1 if p &gt; .5 else 0 i = classes[tuple(y)] j = classes[tuple(predicted)] confusion_matrix[i][j] += 1 test_ctr +=1 if test_ctr &gt;= no_test: break . matplotlib.rcParams[&#39;font.size&#39;] = 15 fig = pt.figure(figsize=(11, 11)) pt.clf() ax = fig.add_subplot(111) ax.set_aspect(1) res = ax.imshow(confusion_matrix, cmap=pt.cm.jet, interpolation=&#39;nearest&#39;) cb = fig.colorbar(res) labels = [{v: k for k, v in classes.iteritems()}[i] for i in range(len(classes))] pt.xticks(range(len(classes)), labels) pt.yticks(range(len(classes)), labels) pt.show() . The confusion matrix shows actual classes along the vertical and predicted classes along the horizontal axis. . The vast majority of observed classes are $(0, 0, 0)$ and our classifiers get most of these right except that some are misclassified as $(0, 0, 1)$ and $(0, 1, 0)$. . All observed bot-related changes (classes starting with a $1$) are $(1, 1, 0)$ (i.e. minor bot-effected changes) and our classifiers get all of those right. .",
            "url": "https://georg.io/2014/10/10/Classifying_Wikipedia_Changes",
            "relUrl": "/2014/10/10/Classifying_Wikipedia_Changes",
            "date": " • Oct 10, 2014"
        }
        
    
  
    
        ,"post8": {
            "title": "Linkage discovery between scientific articles in Python and with graphs",
            "content": "PLOS Biology-Inspired PLOS Biology Articles . This past week I had my first encounter with the concept of graph databases which lend themselves perfectly to modeling and capturing linked data. . I started reading the free and brilliant book Graph Databases by Robinson, Webber, and Eifrem and began playing around with Python bulbs by James Thornton. . I further took the data set of 1754 PLOS Biology articles that I have examined on this blog multiple times and created a Rexster-based graph database from them. Apart from the obvious authors, DOIs, and titles I also extracted references to other PLOS Biology articles. . In this blog post I will examine these links between PLOS Biology articles. . Let us first take a look at my database to get an idea of what this looks like. . %matplotlib inline from matplotlib import pyplot . from bulbs.rexster import Graph, Config, REXSTER_URI . REXSTER_URI = &#39;http://localhost:8182/graphs/plos&#39; config = Config(REXSTER_URI) . g = Graph(config) . The label g now holds a reference to our graph database. . Python bulbs allows us to define classes for our data model which is something I did when creating this graph database in the first place. These are the node (vertex) types and edge (relationship) types I defined: . # Bulbs Models from bulbs.model import Node, Relationship from bulbs.property import String, Integer, DateTime, List class Author(Node): element_type = &#39;author&#39; name = String(nullable=False) class Article(Node): element_type = &#39;article&#39; title = String(nullable=False) published = DateTime() doi = String() class Authorship(Relationship): label = &#39;authored&#39; class Citation(Relationship): label = &#39;cites&#39; reference_count = Integer(nullable=False) tag = String() . This is a very basic model of PLOS Biology articles that captures nothing more than authorship (edges between authors and articles) and citations (edges between articles). . Some of these concepts can and should probably be decorated further: for instance Authorship edges could include author contributions (as provided at the bottom of most PLOS Biology articles). . g.add_proxy(&#39;authors&#39;, Author) g.add_proxy(&#39;articles&#39;, Article) g.add_proxy(&#39;authored&#39;, Authorship) g.add_proxy(&#39;cites&#39;, Citation) . Usually we would use Rexster/Bulbs-builtin functions that rely on some internal index but since that index seems to be broken for me right now I will simply collect all nodes and edges by hand and create Python dictionaries as indeces. . This is okay here to do since our database is very small but would likely be prohibitive for anything marginally bigger. . nodes = g.V edges = g.E . authors = {n.name: n for n in nodes if n.element_type == &#39;author&#39;} . authors.keys()[:10] . [u&#39;Shuguang Zhang&#39;, u&#39;Ernst Hafen&#39;, u&#39;Maren Brockmeyer&#39;, u&#39;Bruno Eschli&#39;, u&#39;David B. Gurevich&#39;, u&#39;Michael Lynch&#39;, u&#39;Alejandro Valbuena&#39;, u&#39;Claudia Rutte&#39;, u&#39;Matthew M Wyatt&#39;, u&#39;Brianna B. Williams&#39;] . articles = {n.doi: n for n in nodes if n.element_type == &#39;article&#39;} . articles.keys()[:10] . [u&#39;10.1371/journal.pbio.0040216&#39;, u&#39;10.1371/journal.pbio.0040215&#39;, u&#39;10.1371/journal.pbio.0040210&#39;, u&#39;10.1371/journal.pbio.0040368&#39;, u&#39;10.1371/journal.pbio.0040369&#39;, u&#39;10.1371/journal.pbio.0040362&#39;, u&#39;10.1371/journal.pbio.0040363&#39;, u&#39;10.1371/journal.pbio.0040360&#39;, u&#39;10.1371/journal.pbio.0020275&#39;, u&#39;10.1371/journal.pbio.0040366&#39;] . Let us now do a brief sanity check and count the number of PLOS Biology articles in our data set (this should equal 1754). . len(articles.keys()) . 1754 . Let us now pick an article at random and see how this article is connected to the remainder of the graph. . article = articles[&#39;10.1371/journal.pbio.1000584&#39;] . This is the title of article: . article.title . u&#39;Clusters of Temporal Discordances Reveal Distinct Embryonic Patterning Mechanisms in Drosophila and Anopheles&#39; . These are the edges pointing to this article: . list(article.inE()) . [&lt;Authorship: http://localhost:8182/graphs/plos/edges/21895&gt;, &lt;Authorship: http://localhost:8182/graphs/plos/edges/21893&gt;, &lt;Authorship: http://localhost:8182/graphs/plos/edges/21891&gt;] . There are three Authorship edges that point to this specific article. . To get the node at the base of a directed edge we can either query article.inE().inV() (i.e. the in-node of this edge) or simply ask for the in-node of the article node straight away - this should be equivalent! . for author in article.inV(): print author.name . Yury Goltsev Michael Levine Dmitri Papatsenko . A quick check online confirms that these are indeed the authors of article. . As I mentioned above, I also collected all references to other PLOS Biology articles in my data set and modeled those as Citation relationships (edges) between articles. . The article we are currently looking at has one such out-edge to another PLOS Biology article: . list(article.outE()) . [&lt;Citation: http://localhost:8182/graphs/plos/edges/29290&gt;] . for citation in article.outV(): print citation.title print [n.name for n in citation.inV() if n.element_type == &#39;author&#39;] print citation.doi . The Cell Cycle–Regulated Genes of Schizosaccharomyces pombe [u&#39;Saumyadipta Pyne&#39;, u&#39;Janet Leatherwood&#39;, u&#39;Anna Oliva&#39;, u&#39;Bruce Futcher&#39;, u&#39;Adam Rosebrock&#39;, u&#39;Steve Skiena&#39;, u&#39;Francisco Ferrezuelo&#39;, u&#39;Haiying Chen&#39;] 10.1371/journal.pbio.0030225 . As you can see above, querying our database for the authors of the PLOS Biology article that our current article (article) cites is simple. . How many PLOS Biology articles in our data set of 1754 articles cite other PLOS Biology articles? . (caveat: this only represents those citations that I detected when parsing my set of articles) . sum(1 for n in nodes if n.element_type == &#39;article&#39; and n.outV() &gt; 0) . 526 . I did not only extract citation edges between PLOS Biology articles but also counted how often such a citation occurs in the body of the article. . For our article and its one cited PLOS Biology article I counted: . for citation in article.outE(): print citation.reference_count . 2 . Just to verify this, look up article online (DOI = 10.1371/journal.pbio.0030225) and look for reference [4] which corresponds to this one cited PLOS Biology article. . Let us now take a look at the observed distribution of how often cited PLOS Biology articles are referenced in the main text of the citing PLOS Biology article. . citation_counts = [] for doi in articles.keys(): if articles[doi].outE(): for e in articles[doi].outE(): if e.label == &#39;cites&#39;: citation_counts.append(e.reference_count) . pyplot.hist(citation_counts, bins=range(20)) pyplot.xlabel(&#39;number of times cited&#39;) pyplot.ylabel(&#39;count&#39;) . &lt;matplotlib.text.Text at 0x9a293d0&gt; . This histogram has a surprisingly long tail. Let us take a look at some of the bigger values to see if these make sense. . def article_pp(article): authors = unicode(&#39;, &#39;.join([n.name for n in article.inV() if n.element_type == &#39;author&#39;])) s = (&#39;Title: %s n&#39; &#39;Authors: %s n&#39; &#39;DOI: %s&#39; % (article.title, authors, article.doi)) return s . for edge in edges: if edge.label == &#39;cites&#39;: if edge.reference_count &gt;= 21: print(&#39;Citer:&#39;) print(article_pp(edge.outV())) print(&#39;&#39;) print(&#39;Citee&#39;) print(article_pp(edge.inV())) print(&#39;&#39;) print(&#39;Citer cites citee %d times.&#39; % edge.reference_count) print(&#39;--&#39;) . Citer: Title: A Feedback Loop between Dynamin and Actin Recruitment during Clathrin-Mediated Endocytosis Authors: Marko Lampe, Christien J. Merrifield, Marcus J. Taylor DOI: 10.1371/journal.pbio.1001302 Citee Title: A High Precision Survey of the Molecular Dynamics of Mammalian Clathrin-Mediated Endocytosis Authors: Marcus J. Taylor, David Perrais, Christien J. Merrifield DOI: 10.1371/journal.pbio.1000604 Citer cites citee 21 times. -- Citer: Title: H2A.Z-Mediated Localization of Genes at the Nuclear Periphery Confers Epigenetic Memory of Previous Transcriptional State Authors: Yvonne Fondufe-Mittendorf, Sara Ahmed, Jason H Brickner, Donna Garvey Brickner, Jonathan Widom, Ivelisse Cajigas, Pei-Chih Lee DOI: 10.1371/journal.pbio.0050081 Citee Title: Gene Recruitment of the Activated INO1 Locus to the Nuclear Membrane Authors: Peter Walter, Jason H Brickner DOI: 10.1371/journal.pbio.0020342 Citer cites citee 21 times. -- . Checking these by hand we verify that our counts are correct. . I think it is sensible to postulate that the more often one article cites another one, the more heavily the work presented in the citer was influenced by the citee. . There is certainly some cut-off at which importance stops increasing - my point is simply that citing another article multiple times in your manuscript probably means that you are basing your work at least partially on the article you cite. . In the above list we can already see that one article titled A sex-ratio Meiotic Drive System in Drosophila simulans. II: An X-linked Distorter is a clear follow-up to the article titled A sex-ratio Meiotic Drive System in Drosophila simulans. I: An Autosomal Suppressor. . One question I am interested in is: How inspired are authors by their own work (generally very inspired I would presume), and how inspiring are articles to a completely different group of authors? . In my opinion, if one group of authors inspires a completely different group of authors to carry out scientific work (be it to follw up, refute, or whatever) then that defines knowledge transfer and a point at which scientific knowledge really becomes worth the time and resources it cost to produce this knowledge in the first place. . (I am certain this statement can be refined further but roughly speaking this is what I think) . Let us redo the above histogram but exclude all cited PLOS Biology articles that have one or more authors in common with the citing article. . (one more bracketed caveat: When constructing my database I assumed that every author name occurs exactly once and is therefore unique - this is a heuristic that breaks easily) . def are_different_authors(article_1, article_2): authors_1 = [] authors_2 = [] for n in article_1.inV(): if n.element_type == &#39;author&#39;: authors_1.append(n.name) for n in article_2.inV(): if n.element_type == &#39;author&#39;: authors_2.append(n.name) authors_1 = set(authors_1) authors_2 = set(authors_2) return len(authors_1.intersection(authors_2)) == 0 . citation_counts = [] for edge in edges: if edge.label == &#39;cites&#39;: if are_different_authors(edge.inV(), edge.outV()): citation_counts.append(edge.reference_count) . pyplot.hist(citation_counts, bins=range(20)) pyplot.xlabel(&#39;number of times cited&#39;) pyplot.ylabel(&#39;count&#39;) . &lt;matplotlib.text.Text at 0xaa25310&gt; . This histogram does not look very different from the one above. . Let us take a look at data points in the tail: . for edge in edges: if edge.label == &#39;cites&#39;: if edge.reference_count &gt;= 16 and are_different_authors(edge.inV(), edge.outV()): print(&#39;Citer:&#39;) print(article_pp(edge.outV())) print(&#39;&#39;) print(&#39;Citee&#39;) print(article_pp(edge.inV())) print(&#39;&#39;) print(&#39;Citer cites citee %d times.&#39; % edge.reference_count) print(&#39;--&#39;) . Citer: Title: Lack of Support for the Association between GAD2 Polymorphisms and Severe Human Obesity Authors: Frank Geller, John P Kane, Raphael Merriman, Christian Vaisse, Winfried Rief, Robert Dent, Johannes Hebebrand, Björn Waldenmaier, Franck Mauvais-Jarvis, Anke Hinney, Michael M Swarbrick, Clive R Pullinger, Mary Malloy, Len A Pennacchio, Anna Ustaszewska, Denise L Lind, Wen-Chi Hsueh, Ruth McPherson, Martha M Cavazos, André Scherag, Pui-Yan Kwok DOI: 10.1371/journal.pbio.0030315 Citee Title: GAD2 on Chromosome 10p12 Is a Candidate Gene for Human Obesity Authors: Lynn Bekris, Valérie Vasseur-Delannoy, Philippe Boutin, Karin Séron, Philippe Froguel, Mohamed Chikri, Christian Dina, Laetitia Corset, M. Aline Charles, Séverine Dubois, Francis Vasseur, Janice Cabellon, Ake Lernmark, Bernadette Neve, Karine Clement DOI: 10.1371/journal.pbio.0000068 Citer cites citee 17 times. -- Citer: Title: Structural Basis of Rap Phosphatase Inhibition by Phr Peptides Authors: Alberto Marina, Francisca Gallego del Sol DOI: 10.1371/journal.pbio.1001511 Citee Title: Structural Basis of Response Regulator Inhibition by a Bacterial Anti-Activator Protein Authors: Matthew B. Neiditch, Melinda D. Baker DOI: 10.1371/journal.pbio.1001226 Citer cites citee 16 times. -- . As we can see the two article pairs in the tail of this updated distribution are linked with lower reference counts than what we observed before filtering for author disjointedness. . Now, how inspiring are PLOS Biology authors for other (different) PLOS Biology authors? . To answer this question, I would like to propose a measure that I have called Inspiration Factor in my own head for some time now and one variant of the model I have had in mind is this: . Inspiration is an increasing function of the number of authors (unrelated to you) that you inspired to carry out scientific work. . Since I do not want to count citations that are mentioned only once in the main text of an article, I will impose a threshold of at least three references. . I should refine the way I parse articles to account for the context that citations are referenced in. . Anyways, let us take a look at those PLOS Biology articles that have inspired at least three other PLOS Biology articles. . inspirators = [] for article in articles.values(): in_nodes = [] if article.inE(): for edge in article.inE(): if edge.label == &#39;cites&#39;: if are_different_authors(edge.inV(), edge.outV()) and edge.reference_count &gt;= 3: in_nodes.append([edge.outV(), edge.reference_count]) if len(in_nodes) &gt;= 3: inspirators.append([article, in_nodes]) . len(inspirators) . 2 . for inspirator in inspirators: print(&#39;Inspirator&#39;) print article_pp(inspirator[0]) print(&#39;&#39;) for el in inspirator[1]: print(&#39;Inspired Article&#39;) print article_pp(el[0]) print(&#39;Cites inspirator %d times.&#39; % el[1]) print(&#39;&#39;) print(&#39;--&#39;) print(&#39;&#39;) . Inspirator Title: The Evolution of Combinatorial Gene Regulation in Fungi Authors: Alexander D Johnson, Aaron D Hernday, Hao Li, Brian B Tuch, David J Galgoczy DOI: 10.1371/journal.pbio.0060038 Inspired Article Title: Biofilm Matrix Regulation by Candida albicans Zap1 Authors: Oliver R. Homann, Clarissa J. Nobile, Jean-Sebastien Deneault, Aaron P. Mitchell, Andre Nantel, Aaron D. Hernday, David R. Andes, Jeniel E. Nett, Alexander D. Johnson DOI: 10.1371/journal.pbio.1000133 Cites inspirator 3 times. Inspired Article Title: Evolutionary Tinkering with Conserved Components of a Transcriptional Regulatory Network Authors: Jaideep Mallick, Adnane Sellam, Hugo Lavoie, Hervé Hogues, Malcolm Whiteway, André Nantel DOI: 10.1371/journal.pbio.1000329 Cites inspirator 6 times. Inspired Article Title: Evolution of Phosphoregulation: Comparison of Phosphorylation Patterns across Yeast Species Authors: Assen Roguev, Dorothea Fiedler, Jonathan C. Trinidad, Wendell A. Lim, Pedro Beltrao, Kevan M. Shokat, Alma L. Burlingame, Nevan J. Krogan DOI: 10.1371/journal.pbio.1000134 Cites inspirator 3 times. -- Inspirator Title: Transcription Factors Bind Thousands of Active and Inactive Regions in the Drosophila Blastoderm Authors: Lisa Simirenko, Michael B Eisen, Mark Stapleton, Richard Weiszmann, Cris L. Luengo Hendriks, Tom Gingeras, Amy Beaton, Hou Cheng Chu, Xiao-yong Li, Terence P Speed, Victor Sementchenko, Mark D Biggin, Richard Bourgon, Stewart MacArthur, William Inwood, Susan E Celniker, Nobuo Ogawa, Venky N Iyer, David W Knowles, Daniel A Pollard, David Nix, Aaron Hechmer DOI: 10.1371/journal.pbio.0060027 Inspired Article Title: Target Genes of the MADS Transcription Factor SEPALLATA3: Integration of Developmental and Hormonal Pathways in the Arabidopsis Flower Authors: Cezary Smaczniak, Kerstin Kaufmann, Pawel Krajewski, Ruy Jauregui, Chiara A Airoldi, Gerco C Angenent, Jose M Muiño DOI: 10.1371/journal.pbio.1000090 Cites inspirator 3 times. Inspired Article Title: Evolutionary Plasticity of Polycomb/Trithorax Response Elements in Drosophila Species Authors: Arne Hauenschild, Leonie Ringrose, Renato Paro, Christina Altmutter, Marc Rehmsmeier DOI: 10.1371/journal.pbio.0060261 Cites inspirator 6 times. Inspired Article Title: Quantitative Analysis of the Drosophila Segmentation Regulatory Network Using Pattern Generating Potentials Authors: Sudhir Kumar, Susan E. Celniker, Ann S. Hammonds, Saurabh Sinha, Majid Kazemian, Charles Blatti, Noriko Wakabayashi-Ito, Scot A. Wolfe, Adam Richards, Michael McCutchan, Michael H. Brodsky DOI: 10.1371/journal.pbio.1000456 Cites inspirator 7 times. -- . And that is it for now. . I will expand my dataset to include more articles and think about how to enrich the data I extract from these articles. . One question that I am very intrigued to tackle soon is: How long of a chain of scientific discovery do you trigger? . I imagine that an article that lies at the beginning of a long chain of articles that inspired one another would have some significance. .",
            "url": "https://georg.io/2014/03/22/PLOS_Biology-Inspired_PLOS_Biology",
            "relUrl": "/2014/03/22/PLOS_Biology-Inspired_PLOS_Biology",
            "date": " • Mar 22, 2014"
        }
        
    
  
    
        ,"post9": {
            "title": "Topic discovery in scientific articles with Python",
            "content": "PLOS Biology Topics . Ever wonder what topics are discussed in PLOS Biology articles? Here I will apply an implementation of Latent Dirichlet Allocation (LDA) on a set of 1,754 PLOS Biology articles to work out what a possible collection of underlying topics could be. . I first read about LDA in Building Machine Learning Systems with Python co-authored by Luis Coelho. . LDA seems to have been first described by Blei et al. and I will use the implementation provided by gensim which was written by Radim Řehůřek. . import gensim . import plospy import os . import nltk . import cPickle as pickle . With the following lines of code we open, parse, and tokenize all 1,754 PLOS Biology articles in our collection. . As this takes a bit of time and memory, I carried out all of these steps once and stored the resulting data structures to my hard disk for later reuse - see further below. . all_names = [name for name in os.listdir(&#39;../plos/plos_biology/plos_biology_data&#39;) if &#39;.dat&#39; in name] . article_bodies = [] for name_i, name in enumerate(all_names): docs = plospy.PlosXml(&#39;../plos/plos_biology/plos_biology_data/&#39;+name) for article in docs.docs: article_bodies.append(article[&#39;body&#39;]) . We have 1,754 PLOS Biology articles in our collection: . len(article_bodies) . punkt_param = nltk.tokenize.punkt.PunktParameters() punkt_param.abbrev_types = set([&#39;et al&#39;, &#39;i.e&#39;, &#39;e.g&#39;, &#39;ref&#39;, &#39;c.f&#39;, &#39;fig&#39;, &#39;Fig&#39;, &#39;Eq&#39;, &#39;eq&#39;, &#39;eqn&#39;, &#39;Eqn&#39;, &#39;dr&#39;, &#39;Dr&#39;]) sentence_splitter = nltk.tokenize.punkt.PunktSentenceTokenizer(punkt_param) . sentences = [] for body in article_bodies: sentences.append(sentence_splitter.tokenize(body)) . articles = [] for body in sentences: this_article = [] for sentence in body: this_article.append(nltk.tokenize.word_tokenize(sentence)) articles.append(this_article) . pickle.dump(articles, open(&#39;plos_biology_articles_tokenized.list&#39;, &#39;w&#39;)) . articles = pickle.load(open(&#39;plos_biology_articles_tokenized.list&#39;, &#39;r&#39;)) . is_stopword = lambda w: len(w) &lt; 4 or w in nltk.corpus.stopwords.words(&#39;english&#39;) . Save each article as one list of tokens and filter out stopwords: . articles_unfurled = [] for article in articles: this_article = [] for sentence in article: this_article += [token.lower().encode(&#39;utf-8&#39;) for token in sentence if not is_stopword(token)] articles_unfurled.append(this_article) . pickle.dump(articles_unfurled, open(&#39;plos_biology_articles_unfurled.list&#39;, &#39;w&#39;)) . articles_unfurled = pickle.load(open(&#39;plos_biology_articles_unfurled.list&#39;, &#39;r&#39;)) . Dictionary and Corpus Creation . Create a dictionary of all words (tokens) that appear in our collection of PLOS Biology articles and create a bag of words object for each article (doc2bow). . dictionary = gensim.corpora.Dictionary(articles_unfurled) . dictionary.save(&#39;plos_biology.dict&#39;) . dictionary = gensim.corpora.dictionary.Dictionary().load(&#39;plos_biology.dict&#39;) . I noticed that the word figure occurs rather frequently in these articles, so let us exclude this and any other words that appear in more than half of the articles in this data set (thanks to Radim for pointing this out to me). . dictionary.filter_extremes() . corpus = [dictionary.doc2bow(article) for article in articles_unfurled] . gensim.corpora.MmCorpus.serialize(&#39;plos_biology_corpus.mm&#39;, corpus) . corpus = gensim.corpora.MmCorpus(&#39;plos_biology_corpus.mm&#39;) . model = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, update_every=1, chunksize=100, passes=2, num_topics=20) . model.save(&#39;plos_biology.lda_model&#39;) . model = gensim.models.ldamodel.LdaModel.load(&#39;plos_biology.lda_model&#39;) . And these are the twenty topics we find in 1,754 PLOS Biology articles: . for topic_i, topic in enumerate(model.print_topics(20)): print(&#39;topic # %d: %s n&#39; % (topic_i+1, topic)) . Topics with Lemmatized Tokens . As we can notice, some of the tokens in the above topics are just singular and plural forms of the same word. . Let us see what topics we find after lemmatizing all of our tokens. . from nltk.stem import WordNetLemmatizer wnl = WordNetLemmatizer() articles_lemmatized = [] for article in articles_unfurled: articles_lemmatized.append([wnl.lemmatize(token) for token in article]) . pickle.dump(articles_lemmatized, open(&#39;plos_biology_articles_lemmatized.list&#39;, &#39;w&#39;)) . dictionary_lemmatized = gensim.corpora.Dictionary(articles_lemmatized) . dictionary_lemmatized.save(&#39;plos_biology_lemmatized.dict&#39;) . dictionary_lemmatized.filter_extremes() . corpus_lemmatized = [dictionary_lemmatized.doc2bow(article) for article in articles_lemmatized] . gensim.corpora.MmCorpus.serialize(&#39;plos_biology_corpus_lemmatized.mm&#39;, corpus_lemmatized) . model_lemmatized = gensim.models.ldamodel.LdaModel(corpus_lemmatized, id2word=dictionary_lemmatized, update_every=1, chunksize=100, passes=2, num_topics=20) . for topic_i, topic in enumerate(model_lemmatized.print_topics(20)): print(&#39;topic # %d: %s n&#39; % (topic_i+1, topic)) .",
            "url": "https://georg.io/2014/02/16/PLOS_Biology_Topics",
            "relUrl": "/2014/02/16/PLOS_Biology_Topics",
            "date": " • Feb 16, 2014"
        }
        
    
  
    
        ,"post10": {
            "title": "The Crank-Nicolson method combined with Runge-Kutta implemented from scratch in Python",
            "content": "Combining Crank-Nicolson and Runge-Kutta to Solve a Reaction-Diffusion System . We have already derived the Crank-Nicolson method to integrate the following reaction-diffusion system numerically: . $$ frac{ partial u}{ partial t} = D frac{ partial^2 u}{ partial x^2} + f(u),$$ . $$ frac{ partial u}{ partial x} Bigg|_{x = 0, L} = 0.$$ . Please refer to the earlier blog post for details. . In our previous derivation, we constructed the following stencil that we would go on to rearrange into a system of linear equations that we needed to solve every time step: . $$ frac{U_j^{n+1} - U_j^n}{ Delta t} = frac{D}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right) + f(U_j^n),$$ . where $j$ and $n$ are space and time grid points respectively. . Rearranging the above set of equations, we effectively integrate the reaction part with the explicit Euler method like so: . $$U_j^{n+1} = U_j^n + Delta t f(U_j^n).$$ . For functions $f$ that change rapidly for small changes in their input (stiff equations), using the explicit Euler method may pose stability problems unless we choose a sufficiently small $ Delta t$. . Therefore, I have been wondering if it would be possible to use a more sophisticated and stable numerical scheme to integrate the reaction part in the context of our Crank-Nicolson scheme. . For instance, to integrate the reaction part with the classical Runge-Kutta method, we would write out the following set of equations instead of the aforementioned one: . $$ frac{U_j^{n+1} - U_j^n}{ Delta t} = frac{D}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right) + frac{1}{6} left(k_1 + 2 k_2 + 2 k_3 + k_4 right),$$ . where . $$k_1 = f(U_j^n),$$ . $$k_2 = f left( U_j^n + frac{ Delta t}{2} k_1 right),$$ . $$k_3 = f left( U_j^n + frac{ Delta t}{2} k_2 right),$$ . $$k_4 = f left( U_j^n + Delta t k_3 right).$$ . Whether or not doing this makes sense theoretically I am not certain. But going ahead and implementing this to the numerical example we discussed earlier seems to suggest that this does work. . In the following Python code that is mostly a copy of our previous code we compare the time behaviour and accuracy (measured by mass conservation as our reaction diffusion system preserves mass) of the explicit Euler and Runge-Kutta 4 reaction integration. . We realize that the differences between the obtained numerical results are negligible and we shall compare both approaches with a stiffer reaction term another time. . We shall also take a look at more sophisticated measures of numerical stability another time. . %matplotlib inline import numpy from matplotlib import pyplot . numpy.set_printoptions(precision=3) . L = 1. J = 200 dx = float(L)/float(J-1) x_grid = numpy.array([j*dx for j in range(J)]) . T = 500 N = 1000 dt = float(T)/float(N-1) t_grid = numpy.array([n*dt for n in range(N)]) . D_v = float(10.)/float(100.) D_u = 0.01 * D_v k0 = 0.067 f = lambda u, v: dt*(v*(k0 + float(u*u)/float(1. + u*u)) - u) g = lambda u, v: -f(u,v) sigma_u = float(D_u*dt)/float((2.*dx*dx)) sigma_v = float(D_v*dt)/float((2.*dx*dx)) total_protein = 2.26 . no_high = 10 U = numpy.array([0.1 for i in range(no_high,J)] + [2. for i in range(0,no_high)]) V = numpy.array([float(total_protein-dx*sum(U))/float(J*dx) for i in range(0,J)]) . Let us take a look at the inhomogeneous initial condition: . pyplot.ylim((0., 2.1)) pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U) pyplot.plot(x_grid, V) pyplot.show() . These are the matrices of our system of linear equations whose derivation we described earlier. . A_u = numpy.diagflat([-sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_u]+[1.+2.*sigma_u for i in range(J-2)]+[1.+sigma_u]) + numpy.diagflat([-sigma_u for i in range(J-1)], 1) B_u = numpy.diagflat([sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_u]+[1.-2.*sigma_u for i in range(J-2)]+[1.-sigma_u]) + numpy.diagflat([sigma_u for i in range(J-1)], 1) A_v = numpy.diagflat([-sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_v]+[1.+2.*sigma_v for i in range(J-2)]+[1.+sigma_v]) + numpy.diagflat([-sigma_v for i in range(J-1)], 1) B_v = numpy.diagflat([sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_v]+[1.-2.*sigma_v for i in range(J-2)]+[1.-sigma_v]) + numpy.diagflat([sigma_v for i in range(J-1)], 1) . Function f_vec_ee returns the explicit Euler time step vector while f_vec_rk returns the vector obtained applying the Runge-Kutta 4 method. . def f_vec_ee(U,V): return numpy.multiply(dt, numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U)) . def f_vec_rk(U, V): f_vec = lambda U, V: numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U) k1 = f_vec(U, V) k2 = f_vec(U + numpy.multiply(dt/2., k1), V - numpy.multiply(dt/2., k1)) k3 = f_vec(U + numpy.multiply(dt/2., k2), V - numpy.multiply(dt/2., k2)) k4 = f_vec(U + numpy.multiply(dt, k3), V - numpy.multiply(dt, k3)) return numpy.multiply(dt/6., k1 + numpy.multiply(2., k2) + numpy.multiply(2., k3) + k4) . U_record_ee = numpy.empty(shape=(N,J)) V_record_ee = numpy.empty(shape=(N,J)) U_record_rk = numpy.empty(shape=(N,J)) V_record_rk = numpy.empty(shape=(N,J)) U_record_ee[0][:] = U[:] V_record_ee[0][:] = V[:] U_record_rk[0][:] = U[:] V_record_rk[0][:] = V[:] for ti in range(1,N): U_record_ee[ti][:] = numpy.linalg.solve(A_u, B_u.dot(U_record_ee[ti-1][:]) + f_vec_ee(U_record_ee[ti-1][:],V_record_ee[ti-1][:])) V_record_ee[ti][:] = numpy.linalg.solve(A_v, B_v.dot(V_record_ee[ti-1][:]) - f_vec_ee(U_record_ee[ti-1][:],V_record_ee[ti-1][:])) U_record_rk[ti][:] = numpy.linalg.solve(A_u, B_u.dot(U_record_rk[ti-1][:]) + f_vec_rk(U_record_rk[ti-1][:],V_record_rk[ti-1][:])) V_record_rk[ti][:] = numpy.linalg.solve(A_v, B_v.dot(V_record_rk[ti-1][:]) - f_vec_rk(U_record_rk[ti-1][:],V_record_rk[ti-1][:])) . The initial protein mass in our system: . print &#39;Explicit Euler&#39;, numpy.sum(numpy.multiply(dx, U_record_ee[0]) + numpy.multiply(dx, V_record_ee[0])) print &#39;Runge-Kutta 4 &#39;, numpy.sum(numpy.multiply(dx, U_record_ee[0]) + numpy.multiply(dx, V_record_ee[0])) . Since our reaction-diffusion system preserves mass, we should retain the same protein mass at steady-state for both numerical approaches: . print &#39;Explicit Euler %.14f&#39; % numpy.sum(numpy.multiply(dx, U_record_ee[-1]) + numpy.multiply(dx, V_record_ee[-1])) print &#39;Runge-Kutta 4 %.14f&#39; % numpy.sum(numpy.multiply(dx, U_record_rk[-1]) + numpy.multiply(dx, V_record_rk[-1])) . We realize that the difference between the two numerical methods is neglibible and we shall compare both approaches for a stiffer system another time. . A plot of the steady-state concentration profiles confirms that we cannot observe a significant differences between the results generated by both methods (varying J and N paints the same pictures). . pyplot.ylim((0., 2.1)) pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U_record_ee[-1]) pyplot.plot(x_grid, V_record_ee[-1]) pyplot.plot(x_grid, U_record_rk[-1]) pyplot.plot(x_grid, V_record_rk[-1]) pyplot.show() . Kymograph of U integrated with the explicit Euler method. . fig, ax = pyplot.subplots() pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;t&#39;) pyplot.ylim((0., T)) heatmap = ax.pcolormesh(x_grid, t_grid, U_record_ee, vmin=0., vmax=1.2) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;concentration U&#39;) . Kymograph of U integrated with the Runge-Kutta 4 method. . fig, ax = pyplot.subplots() pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;t&#39;) pyplot.ylim((0., T)) heatmap = ax.pcolormesh(x_grid, t_grid, U_record_rk, vmin=0., vmax=1.2) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;concentration U&#39;) .",
            "url": "https://georg.io/2014/01/09/Crank_Nicolson_Runge_Kutta",
            "relUrl": "/2014/01/09/Crank_Nicolson_Runge_Kutta",
            "date": " • Jan 9, 2014"
        }
        
    
  
    
        ,"post11": {
            "title": "The Crank-Nicolson method applied to an accelerating domain in Python",
            "content": "Reaction Diffusion System on an Accelerating Domain . In a previous blog post we derived equations that describe the time behaviour of a reaction-diffusion system on a growing space domain. . In that work we assumed that the velocity of domain growth is an increasing function of one of the two unknown variables (here, protein concentrations) described by our reaction diffusion system. . Let us add another layer to this problem and assume that not the growth velocity is dependend on the unknown protein concentration but that the growth acceleration is a function of this unknown concentration. . We derived the equations that describe the time dynamics of our previous (velocity) problem in material coordinates here and reproduce them for clarity: . $$ frac{ partial u}{ partial t} = frac{D_u}{g^2} frac{ partial^2 u}{ partial X^2} - left( D_u frac{g_X}{g^3} + frac{a}{g} right) frac{ partial u}{ partial X} + f(u,v) - frac{a_X}{g} u,$$ . $$ frac{ partial v}{ partial t} = frac{D_v}{g^2} frac{ partial^2 v}{ partial X^2} - left( D_v frac{g_X}{g^3} + frac{a}{g} right) frac{ partial v}{ partial X} - f(u,v) - frac{a_X}{g} v,$$ . $$ frac{ partial g}{ partial t} = frac{ partial a}{ partial X},$$ . taken with initial and boundary conditions for $u$, $v$, and $g$ as described earlier. . Let us now introduce a fourth equation to this system to describe the time behaviour of the growth velocity $a$ as a function of its acceleration: . $$ frac{ partial a}{ partial t} = s(X,t),$$ . where $s(X,t)$ is the local growth acceleration. . In this expanded system of partial differential equations the time behaviour of the, now, unknown variable $a$ is described by an uncoupled partial differential equation. Since there is no spatial coupling in this partial differential equation, we do not need to impose any boundary conditions. As initial condition, we will assume that the system is at rest at first: . $$a(X,0) = 0.$$ . Let us now modify our previous code to simulate this expanded system. . Most of this code is just a copy-and-paste of our previous code so please refer to our comments there if anything is unclear. . %matplotlib inline . import numpy from scipy import integrate from matplotlib import pyplot numpy.set_printoptions(precision=3) . L = 1. J = 100 dX = float(L)/float(J) X_grid = numpy.array([float(dX)/2. + j*dX for j in range(J)]) x_grid = numpy.array([j*dX for j in range(J+1)]) T = 200 N = 1000 dt = float(T)/float(N-1) t_grid = numpy.array([n*dt for n in range(N)]) . D_v = float(10.)/float(100.) D_u = 0.01 * D_v . k0 = 0.067 f_vec = lambda U, V: numpy.multiply(dt, numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U)) . The initial condition for the growth velocity a is set to zero everywhere so that the space domain is at rest at first. . The expression for the growth acceleration s is the exact same expression we used earlier for the growth velocity. . total_protein = 2.26 no_high = 10 U = numpy.array([0.1 for i in range(no_high,J)] + [2. for i in range(0,no_high)]) V = numpy.array([float(total_protein-dX*sum(U))/float(J*dX) for i in range(0,J)]) g = numpy.array([1. for j in range(J)]) a = numpy.array([0. for j in range(J)]) s = lambda U: numpy.array([0.001*X_grid[j]*U[j] for j in range(J)]) . This is the polarized initial condition we choose for our concentration system U and V: . pyplot.ylim((0., 2.1)) pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;concentration&#39;) pyplot.plot(X_grid, U) pyplot.plot(X_grid, V) pyplot.show() . As before, the initial condition for the slope g of the trajectories $G$ is 1. everywhere: . pyplot.plot(X_grid, g) pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;g&#39;) pyplot.show() . Initially, s looks the same as a did in our previous code, while in this version of our code a is set initially to 0. everywhere: . pyplot.plot(X_grid, a) pyplot.plot(X_grid, s(U)) pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;g&#39;) pyplot.ylim((-.0001, 0.0021)) pyplot.show() . # syntax to get one element in an array: http://stackoverflow.com/a/7332880/3078529 a_left = lambda a: numpy.concatenate((a[1:J], a[J-1:J])) a_right = lambda a: numpy.concatenate((a[0:1], a[0:J-1])) . f_vec_u = lambda U, V, g, a: numpy.subtract(f_vec(U, V), numpy.multiply(numpy.subtract(a_left(a), a_right(a)), numpy.multiply(float(dt)/(float(2.*dX)), numpy.divide(U,g)))) f_vec_v = lambda U, V, g, a: numpy.subtract(numpy.multiply(-1., f_vec(U, V)), numpy.multiply(numpy.subtract(a_left(a), a_right(a)), numpy.multiply(float(dt)/(float(2.*dX)), numpy.divide(V,g)))) . sigma_u_func = lambda g: numpy.divide(float(D_u*dt)/float(2.*dX*dX), numpy.multiply(g, g)) sigma_v_func = lambda g: numpy.divide(float(D_v*dt)/float(2.*dX*dX), numpy.multiply(g, g)) . # syntax to get one element in an array: http://stackoverflow.com/a/7332880/3078529 g_left = lambda g: numpy.concatenate((g[1:J], g[J-1:J])) g_right = lambda g: numpy.concatenate((g[0:1], g[0:J-1])) . rho_u_func = lambda g, a: numpy.multiply(float(-dt)/float(4.*dX), numpy.add(numpy.divide(a, g), numpy.multiply(numpy.subtract(g_left(g), g_right(g)), numpy.divide(float(D_u)/(2.*dX), numpy.power(g, 3))))) rho_v_func = lambda g, a: numpy.multiply(float(-dt)/float(4.*dX), numpy.add(numpy.divide(a, g), numpy.multiply(numpy.subtract(g_left(g), g_right(g)), numpy.divide(float(D_v)/(2.*dX), numpy.power(g, 3))))) . Note that here we use slightly more meaningful variable names for the ODE steppers for g and x_grid than in our previous code. . The ODE stepper for a is of the same form except that the right-hand side is governed by the growth acceleration s: . g_rhs = lambda t, g, a: numpy.divide(numpy.subtract(a_left(a), a_right(a)), numpy.array([dX]+[2.*dX for j in range(J-2)]+[dX])) g_stepper = integrate.ode(g_rhs) g_stepper = g_stepper.set_integrator(&#39;dopri5&#39;, nsteps=10, max_step=dt) g_stepper = g_stepper.set_initial_value(g, 0.) g_stepper = g_stepper.set_f_params(a) . a_rhs = lambda t, a, s: s a_stepper = integrate.ode(a_rhs) a_stepper = a_stepper.set_integrator(&#39;dopri5&#39;, nsteps=10, max_step=dt) a_stepper = a_stepper.set_initial_value(a, 0.) a_stepper = a_stepper.set_f_params(s(U)) . x_grid_rhs = lambda t, x_grid, a: numpy.concatenate(([0.], numpy.divide(numpy.add(a[0:J-1], a[1:J]), 2.), a[J-1:J])) x_stepper = integrate.ode(x_grid_rhs) x_stepper = x_stepper.set_integrator(&#39;dopri5&#39;, nsteps=10, max_step=dt) x_stepper = x_stepper.set_initial_value(x_grid, 0.) x_stepper = x_stepper.set_f_params(a) . U_record = [] V_record = [] g_record = [] a_record = [] x_record = [] U_record.append(U) V_record.append(V) g_record.append(g) a_record.append(a) x_record.append(x_grid) . Let us now integrate this system for a total of N time points. . Notice that we update the ODE stepper of g, a and x_grid every time step with the current state of their corresponding right-hand-side expressions. This is done with a call to .set_f_params() on the corresponding objects. . for ti in range(1,N): sigma_u = sigma_u_func(g) sigma_v = sigma_v_func(g) rho_u = rho_u_func(g, a) rho_v = rho_v_func(g, a) A_u = numpy.diagflat([-sigma_u[j]+rho_u[j] for j in range(1,J)], -1) + numpy.diagflat([1.+sigma_u[0]+rho_u[0]]+[1.+2.*sigma_u[j] for j in range(1,J-1)]+[1.+sigma_u[J-1]-rho_u[J-1]]) + numpy.diagflat([-(sigma_u[j]+rho_u[j]) for j in range(0,J-1)], 1) B_u = numpy.diagflat([sigma_u[j]-rho_u[j] for j in range(1,J)], -1) + numpy.diagflat([1.-sigma_u[0]-rho_u[0]]+[1.-2.*sigma_u[j] for j in range(1,J-1)]+[1.-sigma_u[J-1]+rho_u[J-1]]) + numpy.diagflat([sigma_u[j]+rho_u[j] for j in range(0,J-1)], 1) A_v = numpy.diagflat([-sigma_v[j]+rho_v[j] for j in range(1,J)], -1) + numpy.diagflat([1.+sigma_v[0]+rho_v[0]]+[1.+2.*sigma_v[j] for j in range(1,J-1)]+[1.+sigma_v[J-1]-rho_v[J-1]]) + numpy.diagflat([-(sigma_v[j]+rho_v[j]) for j in range(0,J-1)], 1) B_v = numpy.diagflat([sigma_v[j]-rho_v[j] for j in range(1,J)], -1) + numpy.diagflat([1.-sigma_v[0]-rho_v[0]]+[1.-2.*sigma_v[j] for j in range(1,J-1)]+[1.-sigma_v[J-1]+rho_v[J-1]]) + numpy.diagflat([sigma_v[j]+rho_v[j] for j in range(0,J-1)], 1) U_new = numpy.linalg.solve(A_u, B_u.dot(U) + f_vec_u(U, V, g, a)) V_new = numpy.linalg.solve(A_v, B_v.dot(V) + f_vec_v(U, V, g, a)) while a_stepper.successful() and a_stepper.t + dt &lt; ti*dt: a_stepper.integrate(a_stepper.t + dt) while g_stepper.successful() and g_stepper.t + dt &lt; ti*dt: g_stepper.integrate(g_stepper.t + dt) while x_stepper.successful() and x_stepper.t + dt &lt; ti*dt: x_stepper.integrate(x_stepper.t + dt) g_stepper = g_stepper.set_f_params(a) x_stepper = x_stepper.set_f_params(a) a_stepper = a_stepper.set_f_params(s(U)) U = U_new V = V_new # these are the correct &quot;y&quot; values to save for the current time step since # we integrate only up to t &lt; ti*dt g = g_stepper.y a = a_stepper.y x_grid = x_stepper.y U_record.append(U) V_record.append(V) g_record.append(g) a_record.append(a) x_record.append(x_grid) . As in our previous code protein mass is conserved by definition of our boundary conditions for U and V. . To make certain that our numerical integration does conserve protein mass (at least to some sensible degree) we work out the initial total mass and final total mass respectively: . sum(numpy.multiply(numpy.diff(x_record[0]),U_record[0]) + numpy.multiply(numpy.diff(x_record[0]),V_record[0])) . sum(numpy.multiply(numpy.diff(x_record[-1]),U_record[-1]) + numpy.multiply(numpy.diff(x_record[-1]),V_record[-1])) . These numbers look good and we therefore assume that our numerical integration is somewhat stable. . A look at a kymograph of the concentration of U reveals that the initial condition on U is lost quickly - which will be due to reasons we discussed earlier. . U_record = numpy.array(U_record) V_record = numpy.array(V_record) fig, ax = pyplot.subplots() pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;t&#39;) heatmap = ax.pcolormesh(x_record[0], t_grid, U_record, vmin=0., vmax=1.2) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;concentration U&#39;) . Since with the above setup we only introduce acceleration and no deceleration, we are not surprised to observe that the growth velocity simply increases over time: . a_record = numpy.array(a_record) fig, ax = pyplot.subplots() pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;t&#39;) heatmap = ax.pcolormesh(X_grid, t_grid, a_record) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;local growth velocity a(X,t)&#39;) . This continual increase in growth velocity is only due to an initial acceleration due to the initial condition we imposed on U: . fig, ax = pyplot.subplots() pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;t&#39;) s_record = numpy.empty(shape=(N, J)) for ti in range(N): s_record[ti] = s(U_record[ti]) heatmap = ax.pcolormesh(X_grid, t_grid, s_record) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;local growth acceleration s&#39;) .",
            "url": "https://georg.io/2014/01/08/Reaction_Diffusion_Accelerating_Domain",
            "relUrl": "/2014/01/08/Reaction_Diffusion_Accelerating_Domain",
            "date": " • Jan 8, 2014"
        }
        
    
  
    
        ,"post12": {
            "title": "Document clustering applied to scientific articles in Python",
            "content": "The Ten Most Similar PLoS Biology Articles . ... at least by some measure. . I recently downloaded 1754 PLoS Biology articles as XML files through the PLoS API and have looked at the distribution of the time to publication of PLoS Biology and other PLoS journals. . Here I will play a little with scikit-learn to see if I can discover those PLoS Biology articles (in my data set) that are most similar to one another. . Import Packages . I started writing a Python package (PLoSPy) for more convient parsing of the XML files I have download from PLoS. . import plospy import os from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import linear_kernel import itertools . Discover Data Files on Hard Disk . all_names = [name for name in os.listdir(&#39;../plos/plos_biology/plos_biology_data&#39;) if &#39;.dat&#39; in name] . all_names[0:10] . print len(all_names) . Vectorize all Articles . To reduce memory use, I wrote the following method that returns an iterator over all article bodies. In passing this iterator to the vectorizer, we avoid loading all articles into memory at once - despite the use of an iterator here, I have not been able to repeat this experiment with all 65,000-odd PLoS ONE articles without running out of memory. . ids = [] titles = [] def get_corpus(all_names): for name_i, name in enumerate(all_names): docs = plospy.PlosXml(&#39;../plos/plos_biology/plos_biology_data/&#39;+name) for article in docs.docs: ids.append(article[&#39;id&#39;]) titles.append(article[&#39;title&#39;]) yield article[&#39;body&#39;] . corpus = get_corpus(all_names) tfidf = TfidfVectorizer().fit_transform(corpus) . Just as a sanity check, the number of DOIs in our data set should now equal 1754 as this is the number of articles I downloaded in the first place. . len(ids) . The vectorizer generated a matrix with 139,748 columns (these are the tokens, i.e. probably unique words used in all 1754 PLoS Biology articles) and 1754 rows (corresponding to individual articles). . tfidf.shape . Let us now compute all pairwise cosine distances betweeen all 1754 vectors (articles) in matrix tfidf. I copied and pasted most of this from a StackOverflow answer that I cannot find now - I will add a link to the answer when I come across it again. . To get the ten most similar articles, we track the top five pairwise matches. . top_five = [[-1,-1,-1] for i in range(5)] threshold = -1. for index in range(len(ids)): cosine_similarities = linear_kernel(tfidf[index:index+1], tfidf).flatten() related_docs_indices = cosine_similarities.argsort()[:-5:-1] first = related_docs_indices[0] second = related_docs_indices[1] if first != index: print &#39;Error&#39; break if cosine_similarities[second] &gt; threshold: if first not in [top[0] for top in top_five] and first not in [top[1] for top in top_five]: scores = [top[2] for top in top_five] replace = scores.index(min(scores)) # print &#39;replace&#39;,replace top_five[replace] = [first, second, cosine_similarities[second]] # print &#39;old threshold&#39;,threshold threshold = min(scores) # print &#39;new threshold&#39;,threshold . The Most Similar Articles . Let us now take a look at the results! . for tf in top_five: print &#39;&#39; print(&#39;Cosine Similarity: %.2f&#39; % tf[2]) print(&#39;Title 1: %s&#39; %titles[tf[0]]) print(&#39;http://www.plosbiology.org/article/info%3Adoi%2F&#39;+str(ids[tf[0]])) print &#39;&#39; print(&#39;Title 2: %s&#39; %titles[tf[1]]) print(&#39;http://www.plosbiology.org/article/info%3Adoi%2F&#39;+str(ids[tf[1]])) print &#39;&#39; .",
            "url": "https://georg.io/2013/12/16/Most_Similar_PLoS_Biology",
            "relUrl": "/2013/12/16/Most_Similar_PLoS_Biology",
            "date": " • Dec 16, 2013"
        }
        
    
  
    
        ,"post13": {
            "title": "The Crank-Nicolson method implemented from scratch in Python",
            "content": "The Crank-Nicolson Method . The Crank-Nicolson method is a well-known finite difference method for the numerical integration of the heat equation and closely related partial differential equations. . We often resort to a Crank-Nicolson (CN) scheme when we integrate numerically reaction-diffusion systems in one space dimension . $$ frac{ partial u}{ partial t} = D frac{ partial^2 u}{ partial x^2} + f(u),$$ . $$ frac{ partial u}{ partial x} Bigg|_{x = 0, L} = 0,$$ . where $u$ is our concentration variable, $x$ is the space variable, $D$ is the diffusion coefficient of $u$, $f$ is the reaction term, and $L$ is the length of our one-dimensional space domain. . Note that we use Neumann boundary conditions and specify that the solution $u$ has zero space slope at the boundaries, effectively prohibiting entrance or exit of material at the boundaries (no-flux boundary conditions). . Finite Difference Methods . Many fantastic textbooks and tutorials have been written about finite difference methods, for instance a free textbook by Lloyd Trefethen. . Here we describe a few basic aspects of finite difference methods. . The above reaction-diffusion equation describes the time evolution of variable $u(x,t)$ in one space dimension ($u$ is a line concentration). If we knew an analytic expression for $u(x,t)$ then we could plot $u$ in a two-dimensional coordinate system with axes $t$ and $x$. . To approximate $u(x,t)$ numerically we discretize this two-dimensional coordinate system resulting, in the simplest case, in a two-dimensional regular grid. This picture is employed commonly when constructing finite differences methods, see for instance Figure 3.2.1 of Trefethen. . Let us discretize both time and space as follows: . $$t_n = n Delta t,~ n = 0, ldots, N-1,$$ . $$x_j = j Delta x,~ j = 0, ldots, J-1,$$ . where $N$ and $J$ are the number of discrete time and space points in our grid respectively. $ Delta t$ and $ Delta x$ are the time step and space step respectively and defined as follows: . $$ Delta t = T / N,$$ . $$ Delta x = L / J,$$ . where $T$ is the point in time up to which we will integrate $u$ numerically. . Our ultimate goal is to construct a numerical method that allows us to approximate the unknonwn analytic solution $u(x,t)$ reasonably well in these discrete grid points. . That is we want construct a method that computes values $U(j Delta x, n Delta t)$ (note: capital $U$) so that . $$U(j Delta x, n Delta t) approx u(j Delta x, n Delta t)$$ . As a shorthand we will write $U_j^n = U(j Delta x, n Delta t)$ and $(j,n)$ to refer to grid point $(j Delta x, n Delta t)$. . The Crank-Nicolson Stencil . Based on the two-dimensional grid we construct we then approximate the operators of our reaction-diffusion system. . For instance, to approximate the time derivative on the left-hand side in grid point $(j,n)$ we use the values of $U$ in two specific grid points: . $$ frac{ partial u}{ partial t} Bigg|_{x = j Delta x, t = n Delta t} approx frac{U_j^{n+1} - U_j^n}{ Delta t}.$$ . We can think of this scheme as a stencil that we superimpose on our $(x,t)$-grid and this particular stencil is commonly referred to as forward difference. . The spatial part of the Crank-Nicolson stencil (or see Table 3.2.2 of Trefethen) for the heat equation ($u_t = u_{xx}$) approximates the Laplace operator of our equation and takes the following form . $$ frac{ partial^2 u}{ partial x^2} Bigg|_{x = j Delta x, t = n Delta t} approx frac{1}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right).$$ . To approximate $f(u(j Delta x, n Delta t))$ we write simply $f(U_j^n)$. . These approximations define the stencil for our numerical method as pictured on Wikipedia. . . Applying this stencil to grid point $(j,n)$ gives us the following approximation of our reaction-diffusion equation: . $$ frac{U_j^{n+1} - U_j^n}{ Delta t} = frac{D}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right) + f(U_j^n).$$ . Reordering Stencil into Linear System . Let us define $ sigma = frac{D Delta t}{2 Delta x^2}$ and reorder the above approximation of our reaction-diffusion equation: . $$- sigma U_{j-1}^{n+1} + (1+2 sigma) U_j^{n+1} - sigma U_{j+1}^{n+1} = sigma U_{j-1}^n + (1-2 sigma) U_j^n + sigma U_{j+1}^n + Delta t f(U_j^n).$$ . This equation makes sense for space indices $j = 1, ldots,J-2$ but it does not make sense for indices $j=0$ and $j=J-1$ (on the boundaries): . $$j=0:~- sigma U_{-1}^{n+1} + (1+2 sigma) U_0^{n+1} - sigma U_{1}^{n+1} = sigma U_{-1}^n + (1-2 sigma) U_0^n + sigma U_{1}^n + Delta t f(U_0^n),$$ . $$j=J-1:~- sigma U_{J-2}^{n+1} + (1+2 sigma) U_{J-1}^{n+1} - sigma U_{J}^{n+1} = sigma U_{J-2}^n + (1-2 sigma) U_{J-1}^n + sigma U_{J}^n + Delta t f(U_{J-1}^n).$$ . The problem here is that the values $U_{-1}^n$ and $U_J^n$ lie outside our grid. . However, we can work out what these values should equal by considering our Neumann boundary condition. Let us discretize our boundary condition at $j=0$ with the backward difference and at $j=J-1$ with the forward difference: . $$ frac{U_1^n - U_0^n}{ Delta x} = 0,$$ . $$ frac{U_J^n - U_{J-1}^n}{ Delta x} = 0.$$ . These two equations make it clear that we need to amend our above numerical approximation for $j=0$ with the identities $U_0^n = U_1^n$ and $U_0^{n+1} = U_1^{n+1}$, and for $j=J-1$ with the identities $U_{J-1}^n = U_J^n$ and $U_{J-1}^{n+1} = U_J^{n+1}$. . Let us reinterpret our numerical approximation of the line concentration of $u$ in a fixed point in time as a vector $ mathbf{U}^n$: . $$ mathbf{U}^n = begin{bmatrix} U_0^n vdots U_{J-1}^n end{bmatrix}.$$Using this notation we can now write our above approximation for a fixed point in time, $t = n Delta t$, compactly as a linear system: . $$ begin{bmatrix} 1+ sigma &amp; - sigma &amp; 0 &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 - sigma &amp; 1+2 sigma &amp; - sigma &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; - sigma &amp; 1+2 sigma &amp; - sigma &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; ddots &amp; ddots &amp; ddots &amp; ddots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; - sigma &amp; 1+2 sigma &amp; - sigma 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; - sigma &amp; 1+ sigma end{bmatrix} begin{bmatrix} U_0^{n+1} U_1^{n+1} U_2^{n+1} vdots U_{J-2}^{n+1} U_{J-1}^{n+1} end{bmatrix} = begin{bmatrix} 1- sigma &amp; sigma &amp; 0 &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 sigma &amp; 1-2 sigma &amp; sigma &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; sigma &amp; 1-2 sigma &amp; sigma &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; ddots &amp; ddots &amp; ddots &amp; ddots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; sigma &amp; 1-2 sigma &amp; sigma 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; sigma &amp; 1- sigma end{bmatrix} begin{bmatrix} U_0^{n} U_1^{n} U_2^{n} vdots U_{J-2}^{n} U_{J-1}^{n} end{bmatrix} + begin{bmatrix} Delta t f(U_0^n) Delta t f(U_1^n) Delta t f(U_2^n) vdots Delta t f(U_{J-2}^n) Delta t f(U_{J-1}^n) end{bmatrix}. $$Note that since our numerical integration starts with a well-defined initial condition at $n=0$, $ mathbf{U}^0$, the vector $ mathbf{U}^{n+1}$ on the left-hand side is the only unknown in this system of linear equations. . Thus, to integrate numerically our reaction-diffusion system from time point $n$ to $n+1$ we need to solve numerically for vector $ mathbf{U}^{n+1}$. . Let us call the matrix on the left-hand side $A$, the one on the right-hand side $B$, and the vector on the right-hand side $ mathbf{f}^n$. Using this notation we can write the above system as . $$A mathbf{U}^{n+1} = B mathbf{U}^n + f^n.$$ . In this linear equation, matrices $A$ and $B$ are defined by our problem: we need to specify these matrices once for our problem and incorporate our boundary conditions in them. Vector $ mathbf{f}^n$ is a function of $ mathbf{U}^n$ and so needs to be reevaluated in every time point $n$. We also need to carry out one matrix-vector multiplication every time point, $B mathbf{U}^n$, and one vector-vector addition, $B mathbf{U}^n + f^n$. . The most expensive numerical operation is inversion of matrix $A$ to solve for $ mathbf{U}^{n+1}$, however we may get away with doing this only once and store the inverse of $A$ as $A^{-1}$: . $$ mathbf{U}^{n+1} = A^{-1} left( B mathbf{U}^n + f^n right).$$ . A Crank-Nicolson Example in Python . Let us apply the CN method to a two-variable reaction-diffusion system that was introduced by Mori et al.: . $$ frac{ partial u}{ partial t} = D_u frac{ partial^2 u}{ partial x^2} + f(u,v),$$ . $$ frac{ partial v}{ partial t} = D_v frac{ partial^2 v}{ partial x^2} - f(u,v),$$ . with Neumann boundary conditions . $$ frac{ partial u}{ partial x} Bigg|_{x=0,L} = 0,$$ . $$ frac{ partial v}{ partial x} Bigg|_{x=0,L} = 0.$$ . The variables of this system, $u$ and $v$, represent the concetrations of the active form and its inactive form respectively. The reaction term $f(u,v)$ describes the interchange (activation and inactivation) between these two states of the protein. A particular property of this system is that the inactive has much greater diffusivity that the active form, $D_v gg D_u$. . Using the CN method to integrate this system numerically, we need to set up two separate approximations . $$A_u mathbf{U}^{n+1} = B_u mathbf{U}^n + mathbf{f}^n,$$ . $$A_v mathbf{V}^{n+1} = B_v mathbf{V}^n - mathbf{f}^n,$$ . with two different $ sigma$ terms, $ sigma_u = frac{D_u Delta t}{2 Delta x^2}$ and $ sigma_v = frac{D_v Delta t}{2 Delta x^2}$. . Import Packages . For the matrix-vector multiplication, vector-vector addition, and matrix inversion that we will need to carry out we will use the Python library NumPy. To visualize our numerical solutions, we will use pyplot. . import numpy from matplotlib import pyplot . Numpy allows us to truncate the numerical values of matrices and vectors to improve their display with set_printoptions. . numpy.set_printoptions(precision=3) . Specify Grid . Our one-dimensional domain has unit length and we define J = 100 equally spaced grid points in this domain. This divides our domain into J-1 subintervals, each of length dx. . L = 1. J = 100 dx = float(L)/float(J-1) x_grid = numpy.array([j*dx for j in range(J)]) . Equally, we define N = 1000 equally spaced grid points on our time domain of length T = 200 thus dividing our time domain into N-1 intervals of length dt. . T = 200 N = 1000 dt = float(T)/float(N-1) t_grid = numpy.array([n*dt for n in range(N)]) . Specify System Parameters and the Reaction Term . We choose our parameter values based on the work by Mori et al.. . D_v = float(10.)/float(100.) D_u = 0.01 * D_v k0 = 0.067 f = lambda u, v: dt*(v*(k0 + float(u*u)/float(1. + u*u)) - u) g = lambda u, v: -f(u,v) sigma_u = float(D_u*dt)/float((2.*dx*dx)) sigma_v = float(D_v*dt)/float((2.*dx*dx)) total_protein = 2.26 . Specify the Initial Condition . As discussed by Mori et al., we can expect to observe interesting behaviour in the steady state of this system if we choose a heterogeneous initial condition for $u$. . Here, we initialize $u$ with a step-like heterogeneity: . no_high = 10 U = numpy.array([0.1 for i in range(no_high,J)] + [2. for i in range(0,no_high)]) V = numpy.array([float(total_protein-dx*sum(u))/float(J*dx) for i in range(0,J)]) . Note that we make certain that total protein amounts equal a certain value, total_protein. The importance of this was discussed by Walther et al.. . Let us plot our initial condition for confirmation: . ylim((0., 2.1)) xlabel(&#39;x&#39;); ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U) pyplot.plot(x_grid, V) pyplot.show() . The blue curve is the initial condition for $U$, stored in Python variable U, and the green curve is the initial condition for $V$ stored in V. . Create Matrices . The matrices that we need to construct are all tridiagonal so they are easy to construct with numpy.diagflat. . A_u = numpy.diagflat([-sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_u]+[1.+2.*sigma_u for i in range(J-2)]+[1.+sigma_u]) + numpy.diagflat([-sigma_u for i in range(J-1)], 1) B_u = numpy.diagflat([sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_u]+[1.-2.*sigma_u for i in range(J-2)]+[1.-sigma_u]) + numpy.diagflat([sigma_u for i in range(J-1)], 1) A_v = numpy.diagflat([-sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_v]+[1.+2.*sigma_v for i in range(J-2)]+[1.+sigma_v]) + numpy.diagflat([-sigma_v for i in range(J-1)], 1) B_v = numpy.diagflat([sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_v]+[1.-2.*sigma_v for i in range(J-2)]+[1.-sigma_v]) + numpy.diagflat([sigma_v for i in range(J-1)], 1) . To confirm, this is what A_u looks like: . print A_u . [[ 1.981 -0.981 0. ..., 0. 0. 0. ] [-0.981 2.962 -0.981 ..., 0. 0. 0. ] [ 0. -0.981 2.962 ..., 0. 0. 0. ] ..., [ 0. 0. 0. ..., 2.962 -0.981 0. ] [ 0. 0. 0. ..., -0.981 2.962 -0.981] [ 0. 0. 0. ..., 0. -0.981 1.981]] . Solve the System Iteratively . To advance our system by one time step, we need to do one matrix-vector multiplication followed by one vector-vector addition on the right hand side. . To facilitate this, we rewrite our reaction term so that it accepts concentration vectors $ mathbf{U}^n$ and $ mathbf{V}^n$ as arguments and returns vector $ mathbf{f}^n$. . As a reminder, this is our non-vectorial definition of $f$ . f = lambda u, v: v*(k0 + float(u*u)/float(1. + u*u)) - u . f_vec = lambda U, V: numpy.multiply(dt, numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U)) . Let us make certain that this produces the same values as our non-vectorial f: . print f(U[0], V[0]) . 0.00996135898275 . print f(U[-1], V[-1]) . -0.0623832232232 . print f_vec(U, V) . [ 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062] . Accounting for rounding of the displayed values due to the set_printoptions we set above, we can see that f and f_vec generate the same values for our initial condition at both ends of our domain. . We will use numpy.linalg.solve to solve our linear system each time step. . While we integrate our system over time we will record both U and V at each time step in U_record and V_record respectively so that we can plot our numerical solutions over time. . U_record = [] V_record = [] U_record.append(U) V_record.append(V) for ti in range(1,N): U_new = numpy.linalg.solve(A_u, B_u.dot(U) + f_vec(U,V)) V_new = numpy.linalg.solve(A_v, B_v.dot(V) - f_vec(U,V)) U = U_new V = V_new U_record.append(U) V_record.append(V) . Plot the Numerical Solution . Let us take a look at the numerical solution we attain after N time steps. . ylim((0., 2.1)) xlabel(&#39;x&#39;); ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U) pyplot.plot(x_grid, V) pyplot.show() . And here is a kymograph of the values of U. This plot shows concisely the behaviour of U over time and we can clear observe the wave-pinning behaviour described by Mori et al.. Furthermore, we observe that this wave pattern is stable for about 50 units of time and we therefore conclude that this wave pattern is a stable steady state of our system. . U_record = numpy.array(U_record) V_record = numpy.array(V_record) fig, ax = subplots() xlabel(&#39;x&#39;); ylabel(&#39;t&#39;) heatmap = ax.pcolor(x_grid, t_grid, U_record, vmin=0., vmax=1.2) .",
            "url": "https://georg.io/2013/12/03/Crank_Nicolson",
            "relUrl": "/2013/12/03/Crank_Nicolson",
            "date": " • Dec 3, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Georg Walther and here I blog about all things tech and data that I find interesting. . For lack of creativity, here is a rundown of the tools I use and projects I do. Maybe someday I’ll come up with a more interesting bio. . See more about me on my LinkedIn page or drop me a line on Twitter. . Industry experience . Data science / machine learning experience . Rare event (e.g. click, conversion, purchase, email open and reaction, generic user action) prediction from user-level time series data using classical machine learning and deep learning | User clustering based on web tracking time series data | Data-driven SEM, display, real-time bidding, and direct marketing optimization | Time-series anomaly detection and reporting for streamed and batched sensor data | Agent-based asset trading (reinforcement learning) | Basket-based recommender systems | Predictive analytics / forecasting of asset demand based on marketplace user behaviour (big data machine learning) | Use of high-dimensional acceleration time series data to solve a classification problem | Causal inference using propensity scores and clustering to estimate treatment effects in high-dimensional acceleration time series data | . DevOps experience . Implementation of two-tiered deployment CI/CD process for a Python/Django/wagtail app | Set up scalable Azure-based infrastructure: Terraform templates for instantiation of managed Kubernetes cluster, database backend and virtual network security; Implementation of Kubernetes deployment and service objects as well as ingress resource and controller | . Full-stack experience . Ideation and implementation of a modern ReactJS web app for the financial sector | . Technologies . Machine Learning &amp; Data Science . Tensorflow, Keras, PyTorch, Numpy, OpenAI Gym, Pandas, Scikit-Learn, Scipy, Spark MLlib . Big Data . Presto, Apache Spark, PySpark, Spark SQL, Spark MLlib . DevOps . Apache Airflow, Ansible, Terraform, Azure Cloud, Google Cloud Platform, Kubernetes, NGINX kubernetes ingress controller, Docker, Docker-Compose, Heroku IaaS, GitLab CI . Frontend . JavaScript, ECMAScript 6, ReactJS . Backend . Apache2, Falcon, Gunicorn, Nginx, PyPy, Pytest, Python, Requests, Django, Wagtail, Flask, Keycloak . Cloud Computing . AWS, Google Cloud Platform, Azure, Azure Managed Kubernetes Service (AKS), AWS Elastic Container Service (ECS), AWS Elastic Kubernetes Service (EKS) . Frameworks / concepts / methods . Anomaly Detection, Continuous Delivery, Continuous Integration, DevOps, Model Selection (AIC, BIC), Reinforcement Learning, Supervised Learning, Unsupervised Learning, Frequency domain feature engineering (fast Fourier transformation / FFT, discrete Fourier transformation / DFT, short-term Fourier transformation / STFT), Causal Inference .",
          "url": "https://georg.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://georg.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}